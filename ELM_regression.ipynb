{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM5kEqBKZt+Ue5Gg52hr/N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MISJasonChuang/Extreme-Machine-Learning/blob/master/ELM_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "2Xb08pGL5gK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-multiflow\n",
        "!pip install statsmodels"
      ],
      "metadata": {
        "id": "I09KmeI3l81M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de11dc4-6d58-4ee6-d4c3-bad6da39ac01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-multiflow\n",
            "  Downloading scikit-multiflow-0.5.3.tar.gz (450 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/450.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/450.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.6/450.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sortedcontainers>=1.5.7 in /usr/local/lib/python3.10/dist-packages (from scikit-multiflow) (2.4.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from scikit-multiflow) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-multiflow) (1.11.4)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-multiflow) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from scikit-multiflow) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from scikit-multiflow) (1.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->scikit-multiflow) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->scikit-multiflow) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->scikit-multiflow) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->scikit-multiflow) (1.16.0)\n",
            "Building wheels for collected packages: scikit-multiflow\n",
            "  Building wheel for scikit-multiflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-multiflow: filename=scikit_multiflow-0.5.3-cp310-cp310-linux_x86_64.whl size=1254682 sha256=2e95efd854f870f801afa2d4896468207759b55aec7844df135785cc5d97537f\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/1b/56/45b17a6cf203d98000a45976cb0dd0c4c3f11960e6a505f231\n",
            "Successfully built scikit-multiflow\n",
            "Installing collected packages: scikit-multiflow\n",
            "Successfully installed scikit-multiflow-0.5.3\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.23.5)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->statsmodels) (2023.3.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
        "!tar -xzvf ta-lib-0.4.0-src.tar.gz\n",
        "%cd ta-lib\n",
        "!./configure --prefix=/usr\n",
        "!make\n",
        "!make install\n",
        "!pip install Ta-Lib"
      ],
      "metadata": {
        "id": "7eK6S-Llt4IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf8\n",
        "# Author: David C. Lambert [dcl -at- panix -dot- com]\n",
        "# Copyright(c) 2013\n",
        "# License: Simple BSD\n",
        "\n",
        "\"\"\"The :mod:`random_layer` module\n",
        "implements Random Layer transformers.\n",
        "\n",
        "Random layers are arrays of hidden unit activations that are\n",
        "random functions of input activation values (dot products for simple\n",
        "activation functions, distances from prototypes for radial basis\n",
        "functions).\n",
        "\n",
        "They are used in the implementation of Extreme Learning Machines (ELMs),\n",
        "but can be used as a general input mapping.\n",
        "\"\"\"\n",
        "\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from scipy.spatial.distance import cdist, pdist, squareform\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.utils.extmath import safe_sparse_dot\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "__all__ = [\n",
        "    \"RandomLayer\",\n",
        "    \"MLPRandomLayer\",\n",
        "    \"RBFRandomLayer\",\n",
        "    \"GRBFRandomLayer\",\n",
        "]\n",
        "\n",
        "\n",
        "class BaseRandomLayer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Abstract Base Class for random  layers\"\"\"\n",
        "\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    _internal_activation_funcs = dict()\n",
        "\n",
        "    @classmethod\n",
        "    def activation_func_names(cls):\n",
        "        \"\"\"Get list of internal activation function names\"\"\"\n",
        "        return cls._internal_activation_funcs.keys()\n",
        "\n",
        "    # take n_hidden and random_state, init components_ and\n",
        "    # input_activations_\n",
        "    def __init__(\n",
        "        self, n_hidden=20, random_state=0, activation_func=None, activation_args=None\n",
        "    ):\n",
        "        self.n_hidden = n_hidden\n",
        "        self.random_state = random_state\n",
        "        self.activation_func = activation_func\n",
        "        self.activation_args = activation_args\n",
        "\n",
        "        self.components_ = dict()\n",
        "        self.input_activations_ = None\n",
        "\n",
        "        # keyword args for internally defined funcs\n",
        "        self._extra_args = dict()\n",
        "\n",
        "    @abstractmethod\n",
        "    def _generate_components(self, X):\n",
        "        \"\"\"Generate components of hidden layer given X\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def _compute_input_activations(self, X):\n",
        "        \"\"\"Compute input activations given X\"\"\"\n",
        "\n",
        "    # compute input activations and pass them\n",
        "    # through the hidden layer transfer functions\n",
        "    # to compute the transform\n",
        "    def _compute_hidden_activations(self, X):\n",
        "        \"\"\"Compute hidden activations given X\"\"\"\n",
        "\n",
        "        self._compute_input_activations(X)\n",
        "\n",
        "        acts = self.input_activations_\n",
        "\n",
        "        if callable(self.activation_func):\n",
        "            args_dict = self.activation_args if (self.activation_args) else {}\n",
        "            X_new = self.activation_func(acts, **args_dict)\n",
        "        else:\n",
        "            func_name = self.activation_func\n",
        "            func = self._internal_activation_funcs[func_name]\n",
        "\n",
        "            X_new = func(acts, **self._extra_args)\n",
        "\n",
        "        return X_new\n",
        "\n",
        "    # perform fit by generating random components based\n",
        "    # on the input array\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Generate a random hidden layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training set: only the shape is used to generate random component\n",
        "            values for hidden units\n",
        "\n",
        "        y : is not used: placeholder to allow for usage in a Pipeline.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self\n",
        "        \"\"\"\n",
        "        X = check_array(X)\n",
        "\n",
        "        self._generate_components(X)\n",
        "\n",
        "        return self\n",
        "\n",
        "    # perform transformation by calling compute_hidden_activations\n",
        "    # (which will normally call compute_input_activations first)\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Generate the random hidden layer's activations given X as input.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
        "            Data to transform\n",
        "\n",
        "        y : is not used: placeholder to allow for usage in a Pipeline.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        X_new : numpy array of shape [n_samples, n_components]\n",
        "        \"\"\"\n",
        "        X = check_array(X)\n",
        "\n",
        "        if self.components_ is None:\n",
        "            raise ValueError(\"No components initialized\")\n",
        "\n",
        "        return self._compute_hidden_activations(X)\n",
        "\n",
        "\n",
        "class RandomLayer(BaseRandomLayer):\n",
        "    \"\"\"RandomLayer is a transformer that creates a feature mapping of the\n",
        "    inputs that corresponds to a layer of hidden units with randomly\n",
        "    generated components.\n",
        "\n",
        "    The transformed values are a specified function of input activations\n",
        "    that are a weighted combination of dot product (multilayer perceptron)\n",
        "    and distance (rbf) activations:\n",
        "\n",
        "      input_activation = alpha * mlp_activation + (1-alpha) * rbf_activation\n",
        "\n",
        "      mlp_activation(x) = dot(x, weights) + bias\n",
        "      rbf_activation(x) = rbf_width * ||x - center||/radius\n",
        "\n",
        "      alpha and rbf_width are specified by the user\n",
        "\n",
        "      weights and biases are taken from normal distribution of\n",
        "      mean 0 and sd of 1\n",
        "\n",
        "      centers are taken uniformly from the bounding hyperrectangle\n",
        "      of the inputs, and radii are max(||x-c||)/sqrt(n_centers*2)\n",
        "\n",
        "    The input activation is transformed by a transfer function that defaults\n",
        "    to numpy.tanh if not specified, but can be any callable that returns an\n",
        "    array of the same shape as its argument (the input activation array, of\n",
        "    shape [n_samples, n_hidden]).  Functions provided are 'sine', 'tanh',\n",
        "    'tribas', 'inv_tribas', 'sigmoid', 'hardlim', 'softlim', 'gaussian',\n",
        "    'multiquadric', or 'inv_multiquadric'.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `n_hidden` : int, optional (default=20)\n",
        "        Number of units to generate\n",
        "\n",
        "    `alpha` : float, optional (default=0.5)\n",
        "        Mixing coefficient for distance and dot product input activations:\n",
        "        activation = alpha*mlp_activation + (1-alpha)*rbf_width*rbf_activation\n",
        "\n",
        "    `rbf_width` : float, optional (default=1.0)\n",
        "        multiplier on rbf_activation\n",
        "\n",
        "    `user_components`: dictionary, optional (default=None)\n",
        "        dictionary containing values for components that woud otherwise be\n",
        "        randomly generated.  Valid key/value pairs are as follows:\n",
        "           'radii'  : array-like of shape [n_hidden]\n",
        "           'centers': array-like of shape [n_hidden, n_features]\n",
        "           'biases' : array-like of shape [n_hidden]\n",
        "           'weights': array-like of shape [n_features, n_hidden]\n",
        "\n",
        "    `activation_func` : {callable, string} optional (default='tanh')\n",
        "        Function used to transform input activation\n",
        "\n",
        "        It must be one of 'tanh', 'sine', 'tribas', 'inv_tribas',\n",
        "        'sigmoid', 'hardlim', 'softlim', 'gaussian', 'multiquadric',\n",
        "        'inv_multiquadric' or a callable.  If None is given, 'tanh'\n",
        "        will be used.\n",
        "\n",
        "        If a callable is given, it will be used to compute the activations.\n",
        "\n",
        "    `activation_args` : dictionary, optional (default=None)\n",
        "        Supplies keyword arguments for a callable activation_func\n",
        "\n",
        "    `random_state`  : int, RandomState instance or None (default=None)\n",
        "        Control the pseudo random number generator used to generate the\n",
        "        hidden unit weights at fit time.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `input_activations_` : numpy array of shape [n_samples, n_hidden]\n",
        "        Array containing dot(x, hidden_weights) + bias for all samples\n",
        "\n",
        "    `components_` : dictionary containing two keys:\n",
        "        `bias_weights_`   : numpy array of shape [n_hidden]\n",
        "        `hidden_weights_` : numpy array of shape [n_features, n_hidden]\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    \"\"\"\n",
        "\n",
        "    # triangular activation function\n",
        "    _tribas = lambda x: np.clip(1.0 - np.fabs(x), 0.0, 1.0)\n",
        "\n",
        "    # inverse triangular activation function\n",
        "    _inv_tribas = lambda x: np.clip(np.fabs(x), 0.0, 1.0)\n",
        "\n",
        "    # sigmoid activation function\n",
        "    _sigmoid = lambda x: 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    # hard limit activation function\n",
        "    _hardlim = lambda x: np.array(x > 0.0, dtype=float)\n",
        "\n",
        "    _softlim = lambda x: np.clip(x, 0.0, 1.0)\n",
        "\n",
        "    # gaussian RBF\n",
        "    _gaussian = lambda x: np.exp(-pow(x, 2.0))\n",
        "\n",
        "    # multiquadric RBF\n",
        "    _multiquadric = lambda x: np.sqrt(1.0 + pow(x, 2.0))\n",
        "\n",
        "    # inverse multiquadric RBF\n",
        "    _inv_multiquadric = lambda x: 1.0 / (np.sqrt(1.0 + pow(x, 2.0)))\n",
        "\n",
        "    # internal activation function table\n",
        "    _internal_activation_funcs = {\n",
        "        \"sine\": np.sin,\n",
        "        \"tanh\": np.tanh,\n",
        "        \"tribas\": _tribas,\n",
        "        \"inv_tribas\": _inv_tribas,\n",
        "        \"sigmoid\": _sigmoid,\n",
        "        \"softlim\": _softlim,\n",
        "        \"hardlim\": _hardlim,\n",
        "        \"gaussian\": _gaussian,\n",
        "        \"multiquadric\": _multiquadric,\n",
        "        \"inv_multiquadric\": _inv_multiquadric,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_hidden=20,\n",
        "        alpha=0.5,\n",
        "        random_state=None,\n",
        "        activation_func=\"tanh\",\n",
        "        activation_args=None,\n",
        "        user_components=None,\n",
        "        rbf_width=1.0,\n",
        "    ):\n",
        "        super(RandomLayer, self).__init__(\n",
        "            n_hidden=n_hidden,\n",
        "            random_state=random_state,\n",
        "            activation_func=activation_func,\n",
        "            activation_args=activation_args,\n",
        "        )\n",
        "\n",
        "        if isinstance(self.activation_func, str):\n",
        "            func_names = self._internal_activation_funcs.keys()\n",
        "            if self.activation_func not in func_names:\n",
        "                msg = \"unknown activation function '%s'\" % self.activation_func\n",
        "                raise ValueError(msg)\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.rbf_width = rbf_width\n",
        "        self.user_components = user_components\n",
        "\n",
        "        self._use_mlp_input = self.alpha != 0.0\n",
        "        self._use_rbf_input = self.alpha != 1.0\n",
        "\n",
        "    def _get_user_components(self, key):\n",
        "        \"\"\"Look for given user component\"\"\"\n",
        "        try:\n",
        "            return self.user_components[key]\n",
        "        except (TypeError, KeyError):\n",
        "            return None\n",
        "\n",
        "    def _compute_radii(self):\n",
        "        \"\"\"Generate RBF radii\"\"\"\n",
        "\n",
        "        # use supplied radii if present\n",
        "        radii = self._get_user_components(\"radii\")\n",
        "\n",
        "        # compute radii\n",
        "        if radii is None:\n",
        "            centers = self.components_[\"centers\"]\n",
        "\n",
        "            n_centers = centers.shape[0]\n",
        "            max_dist = np.max(pairwise_distances(centers))\n",
        "            radii = np.ones(n_centers) * max_dist / sqrt(2.0 * n_centers)\n",
        "\n",
        "        self.components_[\"radii\"] = radii\n",
        "\n",
        "    def _compute_centers(self, X, sparse, rs):\n",
        "        \"\"\"Generate RBF centers\"\"\"\n",
        "\n",
        "        # use supplied centers if present\n",
        "        centers = self._get_user_components(\"centers\")\n",
        "\n",
        "        # use points taken uniformly from the bounding\n",
        "        # hyperrectangle\n",
        "        if centers is None:\n",
        "            n_features = X.shape[1]\n",
        "\n",
        "            if sparse:\n",
        "                fxr = xrange(n_features)\n",
        "                cols = [X.getcol(i) for i in fxr]\n",
        "\n",
        "                min_dtype = X.dtype.type(1.0e10)\n",
        "                sp_min = lambda col: np.minimum(min_dtype, np.min(col.data))\n",
        "                min_Xs = np.array(map(sp_min, cols))\n",
        "\n",
        "                max_dtype = X.dtype.type(-1.0e10)\n",
        "                sp_max = lambda col: np.maximum(max_dtype, np.max(col.data))\n",
        "                max_Xs = np.array(map(sp_max, cols))\n",
        "            else:\n",
        "                min_Xs = X.min(axis=0)\n",
        "                max_Xs = X.max(axis=0)\n",
        "\n",
        "            spans = max_Xs - min_Xs\n",
        "            ctrs_size = (self.n_hidden, n_features)\n",
        "            centers = min_Xs + spans * rs.uniform(0.0, 1.0, ctrs_size)\n",
        "\n",
        "        self.components_[\"centers\"] = centers\n",
        "\n",
        "    def _compute_biases(self, rs):\n",
        "        \"\"\"Generate MLP biases\"\"\"\n",
        "\n",
        "        # use supplied biases if present\n",
        "        biases = self._get_user_components(\"biases\")\n",
        "        if biases is None:\n",
        "            b_size = self.n_hidden\n",
        "            biases = rs.normal(size=b_size)\n",
        "\n",
        "        self.components_[\"biases\"] = biases\n",
        "\n",
        "    def _compute_weights(self, X, rs):\n",
        "        \"\"\"Generate MLP weights\"\"\"\n",
        "\n",
        "        # use supplied weights if present\n",
        "        weights = self._get_user_components(\"weights\")\n",
        "        if weights is None:\n",
        "            n_features = X.shape[1]\n",
        "            hw_size = (n_features, self.n_hidden)\n",
        "            weights = rs.normal(size=hw_size)\n",
        "\n",
        "        self.components_[\"weights\"] = weights\n",
        "\n",
        "    def _generate_components(self, X):\n",
        "        \"\"\"Generate components of hidden layer given X\"\"\"\n",
        "\n",
        "        rs = check_random_state(self.random_state)\n",
        "        if self._use_mlp_input:\n",
        "            self._compute_biases(rs)\n",
        "            self._compute_weights(X, rs)\n",
        "\n",
        "        if self._use_rbf_input:\n",
        "            self._compute_centers(X, sp.issparse(X), rs)\n",
        "            self._compute_radii()\n",
        "\n",
        "    def _compute_input_activations(self, X):\n",
        "        \"\"\"Compute input activations given X\"\"\"\n",
        "\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        mlp_acts = np.zeros((n_samples, self.n_hidden))\n",
        "        if self._use_mlp_input:\n",
        "            b = self.components_[\"biases\"]\n",
        "            w = self.components_[\"weights\"]\n",
        "            mlp_acts = self.alpha * (safe_sparse_dot(X, w) + b)\n",
        "\n",
        "        rbf_acts = np.zeros((n_samples, self.n_hidden))\n",
        "        if self._use_rbf_input:\n",
        "            radii = self.components_[\"radii\"]\n",
        "            centers = self.components_[\"centers\"]\n",
        "            scale = self.rbf_width * (1.0 - self.alpha)\n",
        "            rbf_acts = scale * cdist(X, centers) / radii\n",
        "\n",
        "        self.input_activations_ = mlp_acts + rbf_acts\n",
        "\n",
        "\n",
        "class MLPRandomLayer(RandomLayer):\n",
        "    \"\"\"Wrapper for RandomLayer with alpha (mixing coefficient) set\n",
        "    to 1.0 for MLP activations only\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_hidden=20,\n",
        "        random_state=None,\n",
        "        activation_func=\"tanh\",\n",
        "        activation_args=None,\n",
        "        weights=None,\n",
        "        biases=None,\n",
        "    ):\n",
        "        user_components = {\"weights\": weights, \"biases\": biases}\n",
        "        super(MLPRandomLayer, self).__init__(\n",
        "            n_hidden=n_hidden,\n",
        "            random_state=random_state,\n",
        "            activation_func=activation_func,\n",
        "            activation_args=activation_args,\n",
        "            user_components=user_components,\n",
        "            alpha=1.0,\n",
        "        )\n",
        "\n",
        "\n",
        "class RBFRandomLayer(RandomLayer):\n",
        "    \"\"\"Wrapper for RandomLayer with alpha (mixing coefficient) set\n",
        "    to 0.0 for RBF activations only\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_hidden=20,\n",
        "        random_state=None,\n",
        "        activation_func=\"gaussian\",\n",
        "        activation_args=None,\n",
        "        centers=None,\n",
        "        radii=None,\n",
        "        rbf_width=1.0,\n",
        "    ):\n",
        "        user_components = {\"centers\": centers, \"radii\": radii}\n",
        "        super(RBFRandomLayer, self).__init__(\n",
        "            n_hidden=n_hidden,\n",
        "            random_state=random_state,\n",
        "            activation_func=activation_func,\n",
        "            activation_args=activation_args,\n",
        "            user_components=user_components,\n",
        "            rbf_width=rbf_width,\n",
        "            alpha=0.0,\n",
        "        )\n",
        "\n",
        "\n",
        "class GRBFRandomLayer(RBFRandomLayer):\n",
        "    \"\"\"Random Generalized RBF Hidden Layer transformer\n",
        "\n",
        "    Creates a layer of radial basis function units where:\n",
        "\n",
        "       f(a), s.t. a = ||x-c||/r\n",
        "\n",
        "    with c the unit center\n",
        "    and f() is exp(-gamma * a^tau) where tau and r are computed\n",
        "    based on [1]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `n_hidden` : int, optional (default=20)\n",
        "        Number of units to generate, ignored if centers are provided\n",
        "\n",
        "    `grbf_lambda` : float, optional (default=0.05)\n",
        "        GRBF shape parameter\n",
        "\n",
        "    `gamma` : {int, float} optional (default=1.0)\n",
        "        Width multiplier for GRBF distance argument\n",
        "\n",
        "    `centers` : array of shape (n_hidden, n_features), optional (default=None)\n",
        "        If provided, overrides internal computation of the centers\n",
        "\n",
        "    `radii` : array of shape (n_hidden),  optional (default=None)\n",
        "        If provided, overrides internal computation of the radii\n",
        "\n",
        "    `use_exemplars` : bool, optional (default=False)\n",
        "        If True, uses random examples from the input to determine the RBF\n",
        "        centers, ignored if centers are provided\n",
        "\n",
        "    `random_state`  : int or RandomState instance, optional (default=None)\n",
        "        Control the pseudo random number generator used to generate the\n",
        "        centers at fit time, ignored if centers are provided\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `components_` : dictionary containing two keys:\n",
        "        `radii_`   : numpy array of shape [n_hidden]\n",
        "        `centers_` : numpy array of shape [n_hidden, n_features]\n",
        "\n",
        "    `input_activations_` : numpy array of shape [n_samples, n_hidden]\n",
        "        Array containing ||x-c||/r for all samples\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    ELMRegressor, ELMClassifier, SimpleELMRegressor, SimpleELMClassifier,\n",
        "    SimpleRandomLayer\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] Fernandez-Navarro, et al, \"MELM-GRBF: a modified version of the\n",
        "              extreme learning machine for generalized radial basis function\n",
        "              neural networks\", Neurocomputing 74 (2011), 2502-2510\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # def _grbf(acts, taus):\n",
        "    #     \"\"\"GRBF activation function\"\"\"\n",
        "\n",
        "    #     return np.exp(np.exp(-pow(acts, taus)))\n",
        "\n",
        "    _grbf = lambda acts, taus: np.exp(np.exp(-pow(acts, taus)))\n",
        "\n",
        "    _internal_activation_funcs = {\"grbf\": _grbf}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_hidden=20,\n",
        "        grbf_lambda=0.001,\n",
        "        centers=None,\n",
        "        radii=None,\n",
        "        random_state=None,\n",
        "    ):\n",
        "        super(GRBFRandomLayer, self).__init__(\n",
        "            n_hidden=n_hidden,\n",
        "            activation_func=\"grbf\",\n",
        "            centers=centers,\n",
        "            radii=radii,\n",
        "            random_state=random_state,\n",
        "        )\n",
        "\n",
        "        self.grbf_lambda = grbf_lambda\n",
        "        self.dN_vals = None\n",
        "        self.dF_vals = None\n",
        "        self.tau_vals = None\n",
        "\n",
        "    # get centers from superclass, then calculate tau_vals\n",
        "    # according to ref [1]\n",
        "    def _compute_centers(self, X, sparse, rs):\n",
        "        \"\"\"Generate centers, then compute tau, dF and dN vals\"\"\"\n",
        "\n",
        "        super(GRBFRandomLayer, self)._compute_centers(X, sparse, rs)\n",
        "\n",
        "        centers = self.components_[\"centers\"]\n",
        "        sorted_distances = np.sort(squareform(pdist(centers)))\n",
        "        self.dF_vals = sorted_distances[:, -1]\n",
        "        self.dN_vals = sorted_distances[:, 1] / 100.0\n",
        "        # self.dN_vals = 0.0002 * np.ones(self.dF_vals.shape)\n",
        "\n",
        "        tauNum = np.log(np.log(self.grbf_lambda) / np.log(1.0 - self.grbf_lambda))\n",
        "\n",
        "        tauDenom = np.log(self.dF_vals / self.dN_vals)\n",
        "\n",
        "        self.tau_vals = tauNum / tauDenom\n",
        "\n",
        "        self._extra_args[\"taus\"] = self.tau_vals\n",
        "\n",
        "    # get radii according to ref [1]\n",
        "    def _compute_radii(self):\n",
        "        \"\"\"Generate radii\"\"\"\n",
        "\n",
        "        denom = pow(-np.log(self.grbf_lambda), 1.0 / self.tau_vals)\n",
        "        self.components_[\"radii\"] = self.dF_vals / denom"
      ],
      "metadata": {
        "id": "hJs1D2u35nBX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf8\n",
        "# Author: David C. Lambert [dcl -at- panix -dot- com]\n",
        "# Copyright(c) 2013\n",
        "# License: Simple BSD\n",
        "\n",
        "\"\"\"\n",
        "The :mod:`elm` module implements the\n",
        "Extreme Learning Machine Classifiers and Regressors (ELMClassifier,\n",
        "ELMRegressor, SimpleELMRegressor, SimpleELMClassifier).\n",
        "\n",
        "An Extreme Learning Machine (ELM) is a single layer feedforward\n",
        "network with a random hidden layer components and ordinary linear\n",
        "least squares fitting of the hidden->output weights by default.\n",
        "[1][2]\n",
        "\n",
        "References\n",
        "----------\n",
        ".. [1] http://www.extreme-learning-machines.org\n",
        ".. [2] G.-B. Huang, Q.-Y. Zhu and C.-K. Siew, \"Extreme Learning Machine:\n",
        "          Theory and Applications\", Neurocomputing, vol. 70, pp. 489-501,\n",
        "          2006.\n",
        "\"\"\"\n",
        "\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "import numpy as np\n",
        "from scipy.linalg import pinv\n",
        "\n",
        "from sklearn.utils import as_float_array\n",
        "from sklearn.utils.extmath import safe_sparse_dot\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# from random_layer import RandomLayer, MLPRandomLayer\n",
        "\n",
        "__all__ = [\"ELMRegressor\",\n",
        "           \"ELMClassifier\",\n",
        "           \"GenELMRegressor\",\n",
        "           \"GenELMClassifier\"]\n",
        "\n",
        "\n",
        "# BaseELM class, regressor and hidden_layer attributes\n",
        "# and provides defaults for docstrings\n",
        "class BaseELM(BaseEstimator):\n",
        "    \"\"\"\n",
        "    Base class for ELMs.\n",
        "\n",
        "    Warning: This class should not be used directly.\n",
        "    Use derived classes instead.\n",
        "    \"\"\"\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self, hidden_layer, regressor):\n",
        "        self.regressor = regressor\n",
        "        self.hidden_layer = hidden_layer\n",
        "\n",
        "    @abstractmethod\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using X, y as training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "\n",
        "        y : array-like of shape [n_samples, n_outputs]\n",
        "            Target values (class labels in classification, real numbers in\n",
        "            regression)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "            Returns an instance of self.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "class GenELMRegressor(BaseELM, RegressorMixin):\n",
        "    \"\"\"\n",
        "    ELMRegressor is a regressor based on the Extreme Learning Machine.\n",
        "\n",
        "    An Extreme Learning Machine (ELM) is a single layer feedforward\n",
        "    network with a random hidden layer components and ordinary linear\n",
        "    least squares fitting of the hidden->output weights by default.\n",
        "    [1][2]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `hidden_layer` : random_layer instance, optional\n",
        "        (default=MLPRandomLayer(random_state=0))\n",
        "\n",
        "    `regressor`    : regressor instance, optional (default=None)\n",
        "        If provided, this object is used to perform the regression from hidden\n",
        "        unit activations to the outputs and subsequent predictions.  If not\n",
        "        present, an ordinary linear least squares fit is performed\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `coefs_` : numpy array\n",
        "        Fitted regression coefficients if no regressor supplied.\n",
        "\n",
        "    `fitted_` : bool\n",
        "        Flag set when fit has been called already.\n",
        "\n",
        "    `hidden_activations_` : numpy array of shape [n_samples, n_hidden]\n",
        "        Hidden layer activations for last input.\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    RBFRandomLayer, MLPRandomLayer, ELMRegressor, ELMClassifier\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://www.extreme-learning-machines.org\n",
        "    .. [2] G.-B. Huang, Q.-Y. Zhu and C.-K. Siew, \"Extreme Learning Machine:\n",
        "          Theory and Applications\", Neurocomputing, vol. 70, pp. 489-501,\n",
        "              2006.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 hidden_layer=MLPRandomLayer(random_state=0),\n",
        "                 regressor=None):\n",
        "\n",
        "        super(GenELMRegressor, self).__init__(hidden_layer, regressor)\n",
        "\n",
        "        self.coefs_ = None\n",
        "        self.fitted_ = False\n",
        "        self.hidden_activations_ = None\n",
        "\n",
        "    def _fit_regression(self, y):\n",
        "        \"\"\"\n",
        "        fit regression using pseudo-inverse\n",
        "        or supplied regressor\n",
        "        \"\"\"\n",
        "        if (self.regressor is None):\n",
        "            self.coefs_ = safe_sparse_dot(pinv(self.hidden_activations_), y)\n",
        "        else:\n",
        "            self.regressor.fit(self.hidden_activations_, y)\n",
        "\n",
        "        self.fitted_ = True\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using X, y as training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "\n",
        "        y : array-like of shape [n_samples, n_outputs]\n",
        "            Target values (class labels in classification, real numbers in\n",
        "            regression)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "            Returns an instance of self.\n",
        "        \"\"\"\n",
        "        # fit random hidden layer and compute the hidden layer activations\n",
        "        self.hidden_activations_ = self.hidden_layer.fit_transform(X)\n",
        "        # print(self.hidden_activations_)\n",
        "        # print(f\"-------training hidden_activations shape {self.hidden_activations_.shape}------------\")\n",
        "\n",
        "        # solve the regression from hidden activations to outputs\n",
        "        self._fit_regression(as_float_array(y, copy=True))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _get_predictions(self):\n",
        "        \"\"\"get predictions using internal least squares/supplied regressor\"\"\"\n",
        "        if (self.regressor is None):\n",
        "            preds = safe_sparse_dot(self.hidden_activations_, self.coefs_)\n",
        "        else:\n",
        "            preds = self.regressor.predict(self.hidden_activations_)\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "        if (not self.fitted_):\n",
        "            raise ValueError(\"ELMRegressor not fitted\")\n",
        "\n",
        "        # compute hidden layer activations\n",
        "        self.hidden_activations_ = self.hidden_layer.transform(X)\n",
        "        # print(self.hidden_activations_)\n",
        "        # print(f\"-------hidden_activations shape {self.hidden_activations_.shape}------------\")\n",
        "        # compute output predictions for new hidden activations\n",
        "        predictions = self._get_predictions()\n",
        "\n",
        "        return predictions\n",
        "\n",
        "'''\n",
        "class GenELMClassifier(BaseELM, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    GenELMClassifier is a classifier based on the Extreme Learning Machine.\n",
        "\n",
        "    An Extreme Learning Machine (ELM) is a single layer feedforward\n",
        "    network with a random hidden layer components and ordinary linear\n",
        "    least squares fitting of the hidden->output weights by default.\n",
        "    [1][2]\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `hidden_layer` : random_layer instance, optional\n",
        "        (default=MLPRandomLayer(random_state=0))\n",
        "\n",
        "    `binarizer` : LabelBinarizer, optional\n",
        "        (default=LabelBinarizer(-1, 1))\n",
        "\n",
        "    `regressor`    : regressor instance, optional (default=None)\n",
        "        If provided, this object is used to perform the regression from hidden\n",
        "        unit activations to the outputs and subsequent predictions.  If not\n",
        "        present, an ordinary linear least squares fit is performed\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `classes_` : numpy array of shape [n_classes]\n",
        "        Array of class labels\n",
        "\n",
        "    `genelm_regressor_` : ELMRegressor instance\n",
        "        Performs actual fit of binarized values\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    RBFRandomLayer, MLPRandomLayer, ELMRegressor, ELMClassifier\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://www.extreme-learning-machines.org\n",
        "    .. [2] G.-B. Huang, Q.-Y. Zhu and C.-K. Siew, \"Extreme Learning Machine:\n",
        "              Theory and Applications\", Neurocomputing, vol. 70, pp. 489-501,\n",
        "              2006.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 hidden_layer=MLPRandomLayer(random_state=0),\n",
        "                 binarizer=LabelBinarizer(-1, 1),\n",
        "                 regressor=None):\n",
        "\n",
        "        super(GenELMClassifier, self).__init__(hidden_layer, regressor)\n",
        "\n",
        "        self.binarizer = binarizer\n",
        "\n",
        "        self.classes_ = None\n",
        "        self.genelm_regressor_ = GenELMRegressor(hidden_layer, regressor)\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"\n",
        "        This function return the decision function values related to each\n",
        "        class on an array of test vectors X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : array of shape [n_samples, n_classes] or [n_samples,]\n",
        "            Decision function values related to each class, per sample.\n",
        "            In the two-class case, the shape is [n_samples,]\n",
        "        \"\"\"\n",
        "        return self.genelm_regressor_.predict(X)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using X, y as training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "\n",
        "        y : array-like of shape [n_samples, n_outputs]\n",
        "            Target values (class labels in classification, real numbers in\n",
        "            regression)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "            Returns an instance of self.\n",
        "        \"\"\"\n",
        "        self.classes_ = np.unique(y)\n",
        "\n",
        "        y_bin = self.binarizer.fit_transform(y)\n",
        "\n",
        "        self.genelm_regressor_.fit(X, y_bin)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "        raw_predictions = self.decision_function(X)\n",
        "        class_predictions = self.binarizer.inverse_transform(raw_predictions)\n",
        "\n",
        "        return class_predictions\n",
        "'''\n",
        "\n",
        "# ELMRegressor with default RandomLayer\n",
        "class ELMRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"\n",
        "    ELMRegressor is a regressor based on the Extreme Learning Machine.\n",
        "\n",
        "    An Extreme Learning Machine (ELM) is a single layer feedforward\n",
        "    network with a random hidden layer components and ordinary linear\n",
        "    least squares fitting of the hidden->output weights by default.\n",
        "    [1][2]\n",
        "\n",
        "    ELMRegressor is a wrapper for an GenELMRegressor that uses a\n",
        "    RandomLayer and passes the __init__ parameters through\n",
        "    to the hidden layer generated by the fit() method.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `n_hidden` : int, optional (default=20)\n",
        "        Number of units to generate in the SimpleRandomLayer\n",
        "\n",
        "    `alpha` : float, optional (default=0.5)\n",
        "        Mixing coefficient for distance and dot product input activations:\n",
        "        activation = alpha*mlp_activation + (1-alpha)*rbf_width*rbf_activation\n",
        "\n",
        "    `rbf_width` : float, optional (default=1.0)\n",
        "        multiplier on rbf_activation\n",
        "\n",
        "    `activation_func` : {callable, string} optional (default='tanh')\n",
        "        Function used to transform input activation\n",
        "\n",
        "        It must be one of 'tanh', 'sine', 'tribas', 'inv_tribase', 'sigmoid',\n",
        "        'hardlim', 'softlim', 'gaussian', 'multiquadric', 'inv_multiquadric' or\n",
        "        a callable.  If none is given, 'tanh' will be used. If a callable\n",
        "        is given, it will be used to compute the hidden unit activations.\n",
        "\n",
        "    `activation_args` : dictionary, optional (default=None)\n",
        "        Supplies keyword arguments for a callable activation_func\n",
        "\n",
        "    `user_components`: dictionary, optional (default=None)\n",
        "        dictionary containing values for components that woud otherwise be\n",
        "        randomly generated.  Valid key/value pairs are as follows:\n",
        "           'radii'  : array-like of shape [n_hidden]\n",
        "           'centers': array-like of shape [n_hidden, n_features]\n",
        "           'biases' : array-like of shape [n_hidden]\n",
        "           'weights': array-like of shape [n_hidden, n_features]\n",
        "\n",
        "    `regressor`    : regressor instance, optional (default=None)\n",
        "        If provided, this object is used to perform the regression from hidden\n",
        "        unit activations to the outputs and subsequent predictions.  If not\n",
        "        present, an ordinary linear least squares fit is performed\n",
        "\n",
        "    `random_state`  : int, RandomState instance or None (default=None)\n",
        "        Control the pseudo random number generator used to generate the\n",
        "        hidden unit weights at fit time.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `genelm_regressor_` : GenELMRegressor object\n",
        "        Wrapped object that actually performs the fit.\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    RandomLayer, RBFRandomLayer, MLPRandomLayer,\n",
        "    GenELMRegressor, GenELMClassifier, ELMClassifier\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://www.extreme-learning-machines.org\n",
        "    .. [2] G.-B. Huang, Q.-Y. Zhu and C.-K. Siew, \"Extreme Learning Machine:\n",
        "          Theory and Applications\", Neurocomputing, vol. 70, pp. 489-501,\n",
        "              2006.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_hidden=20, alpha=0.5, rbf_width=1.0,\n",
        "                 activation_func='tanh', activation_args=None,\n",
        "                 user_components=None, regressor=None, random_state=None):\n",
        "\n",
        "        self.n_hidden = n_hidden\n",
        "        self.alpha = alpha\n",
        "        self.random_state = random_state\n",
        "        self.activation_func = activation_func\n",
        "        self.activation_args = activation_args\n",
        "        self.user_components = user_components\n",
        "        self.rbf_width = rbf_width\n",
        "        self.regressor = regressor\n",
        "\n",
        "        self._genelm_regressor = None\n",
        "\n",
        "    def _create_random_layer(self):\n",
        "        \"\"\"Pass init params to RandomLayer\"\"\"\n",
        "\n",
        "        return RandomLayer(n_hidden=self.n_hidden,\n",
        "                           alpha=self.alpha, random_state=self.random_state,\n",
        "                           activation_func=self.activation_func,\n",
        "                           activation_args=self.activation_args,\n",
        "                           user_components=self.user_components,\n",
        "                           rbf_width=self.rbf_width)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using X, y as training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "\n",
        "        y : array-like of shape [n_samples, n_outputs]\n",
        "            Target values (class labels in classification, real numbers in\n",
        "            regression)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "            Returns an instance of self.\n",
        "        \"\"\"\n",
        "        rhl = self._create_random_layer()\n",
        "        self._genelm_regressor = GenELMRegressor(hidden_layer=rhl,\n",
        "                                                 regressor=self.regressor)\n",
        "        self._genelm_regressor.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "        if (self._genelm_regressor is None):\n",
        "            raise ValueError(\"SimpleELMRegressor not fitted\")\n",
        "\n",
        "        return self._genelm_regressor.predict(X)\n",
        "\n",
        "'''\n",
        "class ELMClassifier(ELMRegressor):\n",
        "    \"\"\"\n",
        "    ELMClassifier is a classifier based on the Extreme Learning Machine.\n",
        "\n",
        "    An Extreme Learning Machine (ELM) is a single layer feedforward\n",
        "    network with a random hidden layer components and ordinary linear\n",
        "    least squares fitting of the hidden->output weights by default.\n",
        "    [1][2]\n",
        "\n",
        "    ELMClassifier is an ELMRegressor subclass that first binarizes the\n",
        "    data, then uses the superclass to compute the decision function that\n",
        "    is then unbinarized to yield the prediction.\n",
        "\n",
        "    The params for the RandomLayer used in the input transform are\n",
        "    exposed in the ELMClassifier constructor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    `n_hidden` : int, optional (default=20)\n",
        "        Number of units to generate in the SimpleRandomLayer\n",
        "\n",
        "    `activation_func` : {callable, string} optional (default='tanh')\n",
        "        Function used to transform input activation\n",
        "\n",
        "        It must be one of 'tanh', 'sine', 'tribas', 'inv_tribase', 'sigmoid',\n",
        "        'hardlim', 'softlim', 'gaussian', 'multiquadric', 'inv_multiquadric' or\n",
        "        a callable.  If none is given, 'tanh' will be used. If a callable\n",
        "        is given, it will be used to compute the hidden unit activations.\n",
        "\n",
        "    `activation_args` : dictionary, optional (default=None)\n",
        "        Supplies keyword arguments for a callable activation_func\n",
        "\n",
        "    `random_state`  : int, RandomState instance or None (default=None)\n",
        "        Control the pseudo random number generator used to generate the\n",
        "        hidden unit weights at fit time.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    `classes_` : numpy array of shape [n_classes]\n",
        "        Array of class labels\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    RandomLayer, RBFRandomLayer, MLPRandomLayer,\n",
        "    GenELMRegressor, GenELMClassifier, ELMClassifier\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://www.extreme-learning-machines.org\n",
        "    .. [2] G.-B. Huang, Q.-Y. Zhu and C.-K. Siew, \"Extreme Learning Machine:\n",
        "          Theory and Applications\", Neurocomputing, vol. 70, pp. 489-501,\n",
        "              2006.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_hidden=20, alpha=0.5, rbf_width=1.0,\n",
        "                 activation_func='tanh', activation_args=None,\n",
        "                 user_components=None, regressor=None,\n",
        "                 binarizer=LabelBinarizer(-1, 1),\n",
        "                 random_state=None):\n",
        "\n",
        "        super(ELMClassifier, self).__init__(n_hidden=n_hidden,\n",
        "                                            alpha=alpha,\n",
        "                                            random_state=random_state,\n",
        "                                            activation_func=activation_func,\n",
        "                                            activation_args=activation_args,\n",
        "                                            user_components=user_components,\n",
        "                                            rbf_width=rbf_width,\n",
        "                                            regressor=regressor)\n",
        "\n",
        "        self.classes_ = None\n",
        "        self.binarizer = binarizer\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        \"\"\"\n",
        "        This function return the decision function values related to each\n",
        "        class on an array of test vectors X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : array of shape [n_samples, n_classes] or [n_samples,]\n",
        "            Decision function values related to each class, per sample.\n",
        "            In the two-class case, the shape is [n_samples,]\n",
        "        \"\"\"\n",
        "        return super(ELMClassifier, self).predict(X)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model using X, y as training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "            Training vectors, where n_samples is the number of samples\n",
        "            and n_features is the number of features.\n",
        "\n",
        "        y : array-like of shape [n_samples, n_outputs]\n",
        "            Target values (class labels in classification, real numbers in\n",
        "            regression)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "            Returns an instance of self.\n",
        "        \"\"\"\n",
        "        self.classes_ = np.unique(y)\n",
        "\n",
        "        y_bin = self.binarizer.fit_transform(y)\n",
        "\n",
        "        super(ELMClassifier, self).fit(X, y_bin)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict values using the model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        C : numpy array of shape [n_samples, n_outputs]\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "        raw_predictions = self.decision_function(X)\n",
        "        class_predictions = self.binarizer.inverse_transform(raw_predictions)\n",
        "\n",
        "        return class_predictions\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"Force use of accuracy score since we don't inherit\n",
        "           from ClassifierMixin\"\"\"\n",
        "\n",
        "        from sklearn.metrics import accuracy_score\n",
        "        return accuracy_score(y, self.predict(X))\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "BRrNFUzw5X5U",
        "outputId": "1c599b2a-fff2-4c9a-b464-9ce287dc05ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass ELMClassifier(ELMRegressor):\\n    \"\"\"\\n    ELMClassifier is a classifier based on the Extreme Learning Machine.\\n\\n    An Extreme Learning Machine (ELM) is a single layer feedforward\\n    network with a random hidden layer components and ordinary linear\\n    least squares fitting of the hidden->output weights by default.\\n    [1][2]\\n\\n    ELMClassifier is an ELMRegressor subclass that first binarizes the\\n    data, then uses the superclass to compute the decision function that\\n    is then unbinarized to yield the prediction.\\n\\n    The params for the RandomLayer used in the input transform are\\n    exposed in the ELMClassifier constructor.\\n\\n    Parameters\\n    ----------\\n    `n_hidden` : int, optional (default=20)\\n        Number of units to generate in the SimpleRandomLayer\\n\\n    `activation_func` : {callable, string} optional (default=\\'tanh\\')\\n        Function used to transform input activation\\n\\n        It must be one of \\'tanh\\', \\'sine\\', \\'tribas\\', \\'inv_tribase\\', \\'sigmoid\\',\\n        \\'hardlim\\', \\'softlim\\', \\'gaussian\\', \\'multiquadric\\', \\'inv_multiquadric\\' or\\n        a callable.  If none is given, \\'tanh\\' will be used. If a callable\\n        is given, it will be used to compute the hidden unit activations.\\n\\n    `activation_args` : dictionary, optional (default=None)\\n        Supplies keyword arguments for a callable activation_func\\n\\n    `random_state`  : int, RandomState instance or None (default=None)\\n        Control the pseudo random number generator used to generate the\\n        hidden unit weights at fit time.\\n\\n    Attributes\\n    ----------\\n    `classes_` : numpy array of shape [n_classes]\\n        Array of class labels\\n\\n    See Also\\n    --------\\n    RandomLayer, RBFRandomLayer, MLPRandomLayer,\\n    GenELMRegressor, GenELMClassifier, ELMClassifier\\n\\n    References\\n    ----------\\n    .. [1] http://www.extreme-learning-machines.org\\n    .. [2] G.-B. Huang, Q.-Y. Zhu and C.-K. Siew, \"Extreme Learning Machine:\\n          Theory and Applications\", Neurocomputing, vol. 70, pp. 489-501,\\n              2006.\\n    \"\"\"\\n\\n    def __init__(self, n_hidden=20, alpha=0.5, rbf_width=1.0,\\n                 activation_func=\\'tanh\\', activation_args=None,\\n                 user_components=None, regressor=None,\\n                 binarizer=LabelBinarizer(-1, 1),\\n                 random_state=None):\\n\\n        super(ELMClassifier, self).__init__(n_hidden=n_hidden,\\n                                            alpha=alpha,\\n                                            random_state=random_state,\\n                                            activation_func=activation_func,\\n                                            activation_args=activation_args,\\n                                            user_components=user_components,\\n                                            rbf_width=rbf_width,\\n                                            regressor=regressor)\\n\\n        self.classes_ = None\\n        self.binarizer = binarizer\\n\\n    def decision_function(self, X):\\n        \"\"\"\\n        This function return the decision function values related to each\\n        class on an array of test vectors X.\\n\\n        Parameters\\n        ----------\\n        X : array-like of shape [n_samples, n_features]\\n\\n        Returns\\n        -------\\n        C : array of shape [n_samples, n_classes] or [n_samples,]\\n            Decision function values related to each class, per sample.\\n            In the two-class case, the shape is [n_samples,]\\n        \"\"\"\\n        return super(ELMClassifier, self).predict(X)\\n\\n    def fit(self, X, y):\\n        \"\"\"\\n        Fit the model using X, y as training data.\\n\\n        Parameters\\n        ----------\\n        X : {array-like, sparse matrix} of shape [n_samples, n_features]\\n            Training vectors, where n_samples is the number of samples\\n            and n_features is the number of features.\\n\\n        y : array-like of shape [n_samples, n_outputs]\\n            Target values (class labels in classification, real numbers in\\n            regression)\\n\\n        Returns\\n        -------\\n        self : object\\n\\n            Returns an instance of self.\\n        \"\"\"\\n        self.classes_ = np.unique(y)\\n\\n        y_bin = self.binarizer.fit_transform(y)\\n\\n        super(ELMClassifier, self).fit(X, y_bin)\\n\\n        return self\\n\\n    def predict(self, X):\\n        \"\"\"\\n        Predict values using the model\\n\\n        Parameters\\n        ----------\\n        X : {array-like, sparse matrix} of shape [n_samples, n_features]\\n\\n        Returns\\n        -------\\n        C : numpy array of shape [n_samples, n_outputs]\\n            Predicted values.\\n        \"\"\"\\n        raw_predictions = self.decision_function(X)\\n        class_predictions = self.binarizer.inverse_transform(raw_predictions)\\n\\n        return class_predictions\\n\\n    def score(self, X, y):\\n        \"\"\"Force use of accuracy score since we don\\'t inherit\\n           from ClassifierMixin\"\"\"\\n\\n        from sklearn.metrics import accuracy_score\\n        return accuracy_score(y, self.predict(X))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHYT6eqn6F0J",
        "outputId": "b3c3cdc6-3347-4e6d-ae98-9e2c5c7e2164"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "# from talib import abstract\n",
        "# import talib\n",
        "\n",
        "sc = preprocessing.StandardScaler()\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/gold prices 2013-2023.csv', parse_dates=['Date'], index_col='Date')\n",
        "df= df.sort_values(by='Date')\n",
        "df = df.rename(columns={\"Close/Last\": \"close\"})\n",
        "df.columns = df.columns.str.lower()\n",
        "# 使用 resample 進行每五天的平均\n",
        "# result = df.resample('5D').mean()\n",
        "# 用rolling算五天的移動平均線\n",
        "moving_avg = df['close'].rolling(window=5).mean()\n",
        "df.columns = df.columns.str.lower()\n",
        "# replace Nan\n",
        "df['volume'] = df['volume'].interpolate()\n",
        "# 將結果加入到 DataFrame 中\n",
        "# df['moving_average'] = moving_avg\n",
        "\n",
        "df['change'] = df['close'].diff()\n",
        "\n",
        "# 1) lowest and highest prices of the 5 previous days;\n",
        "'''\n",
        "df['high_previous_5_days'] = df['high'].shift(5)\n",
        "df['high_previous_4_days'] = df['high'].shift(4)\n",
        "df['high_previous_3_days'] = df['high'].shift(3)\n",
        "df['high_previous_2_days'] = df['high'].shift(2)\n",
        "df['high_previous_1_days'] = df['high'].shift(1)\n",
        "\n",
        "df['low_previous_5_days'] = df['low'].shift(5)\n",
        "df['low_previous_4_days'] = df['low'].shift(4)\n",
        "df['low_previous_3_days'] = df['low'].shift(3)\n",
        "df['low_previous_2_days'] = df['low'].shift(2)\n",
        "df['low_previous_1_days'] = df['low'].shift(1)\n",
        "\n",
        "\n",
        "# 2) opening and closing prices of the 5 previous days;\n",
        "df['Close_previous_5_days'] = df['close'].shift(5)\n",
        "df['Close_previous_4_days'] = df['close'].shift(4)\n",
        "df['Close_previous_3_days'] = df['close'].shift(3)\n",
        "df['Close_previous_2_days'] = df['close'].shift(2)\n",
        "df['Close_previous_1_days'] = df['close'].shift(1)\n",
        "\n",
        "df['open_previous_5_days'] = df['open'].shift(5)\n",
        "df['open_previous_4_days'] = df['open'].shift(4)\n",
        "df['open_previous_3_days'] = df['open'].shift(3)\n",
        "df['open_previous_2_days'] = df['open'].shift(2)\n",
        "df['open_previous_1_days'] = df['open'].shift(1)\n",
        "'''\n",
        "# 3) EMA of the lowest and highest prices of the 5 previous days;\n",
        "# 4) EMA of the opening and closing prices of the 5 previous days;\n",
        "# 5) BB of the opening and closing prices of the 5 previous days;\n",
        "# 6) BB of the lowest and highest prices of the 5 previous days;\n",
        "# df[\"EMA\"] = abstract.EMA(df, 5)\n",
        "# df[\"BB_upper\"], df[\"BB_middle\"], df[\"BB_lower\"]  = talib.BBANDS(df[\"close\"],5,matype = talib.MA_Type.EMA)\n",
        "\n",
        "# df = df[df.columns.difference(['close', 'volume', 'open', 'low'])]\n",
        "df"
      ],
      "metadata": {
        "id": "52_tcgTbKNfA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "e7d574fe-bbf2-4839-9eb4-30a124cc5772"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             close    volume    open    high     low  change\n",
              "Date                                                        \n",
              "2013-08-19  1365.7  116056.0  1377.1  1384.1  1362.0     NaN\n",
              "2013-08-20  1372.6  130096.0  1364.9  1378.0  1351.6     6.9\n",
              "2013-08-21  1370.1  137350.0  1371.0  1378.9  1359.2    -2.5\n",
              "2013-08-22  1370.8  134493.0  1365.6  1381.4  1354.5     0.7\n",
              "2013-08-23  1395.8  149116.0  1376.1  1399.9  1367.8    25.0\n",
              "...            ...       ...     ...     ...     ...     ...\n",
              "2023-08-11  1946.6  119090.0  1944.9  1953.6  1942.7    -2.3\n",
              "2023-08-14  1944.0  117514.0  1945.6  1948.2  1934.2    -2.6\n",
              "2023-08-15  1935.2  161512.0  1939.4  1944.3  1927.5    -8.8\n",
              "2023-08-16  1928.3  124766.0  1933.1  1938.2  1922.0    -6.9\n",
              "2023-08-17  1915.2  146770.0  1922.4  1933.5  1914.2   -13.1\n",
              "\n",
              "[2539 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea7fba6d-eeba-4993-9f28-1b661c9cffe2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>change</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-08-19</th>\n",
              "      <td>1365.7</td>\n",
              "      <td>116056.0</td>\n",
              "      <td>1377.1</td>\n",
              "      <td>1384.1</td>\n",
              "      <td>1362.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-08-20</th>\n",
              "      <td>1372.6</td>\n",
              "      <td>130096.0</td>\n",
              "      <td>1364.9</td>\n",
              "      <td>1378.0</td>\n",
              "      <td>1351.6</td>\n",
              "      <td>6.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-08-21</th>\n",
              "      <td>1370.1</td>\n",
              "      <td>137350.0</td>\n",
              "      <td>1371.0</td>\n",
              "      <td>1378.9</td>\n",
              "      <td>1359.2</td>\n",
              "      <td>-2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-08-22</th>\n",
              "      <td>1370.8</td>\n",
              "      <td>134493.0</td>\n",
              "      <td>1365.6</td>\n",
              "      <td>1381.4</td>\n",
              "      <td>1354.5</td>\n",
              "      <td>0.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-08-23</th>\n",
              "      <td>1395.8</td>\n",
              "      <td>149116.0</td>\n",
              "      <td>1376.1</td>\n",
              "      <td>1399.9</td>\n",
              "      <td>1367.8</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-08-11</th>\n",
              "      <td>1946.6</td>\n",
              "      <td>119090.0</td>\n",
              "      <td>1944.9</td>\n",
              "      <td>1953.6</td>\n",
              "      <td>1942.7</td>\n",
              "      <td>-2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-08-14</th>\n",
              "      <td>1944.0</td>\n",
              "      <td>117514.0</td>\n",
              "      <td>1945.6</td>\n",
              "      <td>1948.2</td>\n",
              "      <td>1934.2</td>\n",
              "      <td>-2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-08-15</th>\n",
              "      <td>1935.2</td>\n",
              "      <td>161512.0</td>\n",
              "      <td>1939.4</td>\n",
              "      <td>1944.3</td>\n",
              "      <td>1927.5</td>\n",
              "      <td>-8.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-08-16</th>\n",
              "      <td>1928.3</td>\n",
              "      <td>124766.0</td>\n",
              "      <td>1933.1</td>\n",
              "      <td>1938.2</td>\n",
              "      <td>1922.0</td>\n",
              "      <td>-6.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-08-17</th>\n",
              "      <td>1915.2</td>\n",
              "      <td>146770.0</td>\n",
              "      <td>1922.4</td>\n",
              "      <td>1933.5</td>\n",
              "      <td>1914.2</td>\n",
              "      <td>-13.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2539 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea7fba6d-eeba-4993-9f28-1b661c9cffe2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ea7fba6d-eeba-4993-9f28-1b661c9cffe2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ea7fba6d-eeba-4993-9f28-1b661c9cffe2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-02ea4dd1-a3b8-4930-9104-31f2cfeceb29\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-02ea4dd1-a3b8-4930-9104-31f2cfeceb29')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-02ea4dd1-a3b8-4930-9104-31f2cfeceb29 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JfzPYjvb4ocM"
      },
      "outputs": [],
      "source": [
        "# from elm import ELMRegressor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "from sklearn import preprocessing\n",
        "from skmultiflow.drift_detection import DDM\n",
        "import statsmodels.api as sm\n",
        "import sys\n",
        "# import talib\n",
        "# from talib import abstract\n",
        "from typing import Union\n",
        "\n",
        "sc = preprocessing.StandardScaler()\n",
        "\n",
        "'''\n",
        "    (Cavalcante & Oliveira, 2015)\n",
        "    Feature Selection:\n",
        "    The input data is composed by daily open, high, low and close prices and two classical technical indicators,\n",
        "    the exponential moving average (EMA) and the Bollinger Bands (BB):\n",
        "    1) lowest and highest prices of the 5 previous days;\n",
        "    2) opening and closing prices of the 5 previous days;\n",
        "    3) EMA of the lowest and highest prices of the 5 previous days;\n",
        "    4) EMA of the opening and closing prices of the 5 previous days;\n",
        "    5) BB of the opening and closing prices of the 5 previous days;\n",
        "    6) BB of the lowest and highest prices of the 5 previous days;\n",
        "'''\n",
        "\n",
        "'''\n",
        "    (Mishra & Das, 2022)\n",
        "    Feature Selection:\n",
        "    The available features for the GOLD/USD dataset are price, open price, high price, low price, volume, and price change.\n",
        "    The gold data available for predictions are in 1 Troy Ounce Unit.\n",
        "    This experimentation has worked out for four time horizons such as 1 day, 3 days, 7 days, and 1 month in advance.\n",
        "    A total of 70% of data is selected for training and 30% is selected for testing .\n",
        "'''\n",
        "def handle_datasets(show_fig: bool = False) -> list:\n",
        "    # reverse dataframe datetime\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/gold prices 2013-2023.csv', parse_dates=['Date'], index_col='Date')\n",
        "    df= df.sort_values(by='Date')\n",
        "    df = df.rename(columns={\"Close/Last\": \"close\"})\n",
        "    df.columns = df.columns.str.lower()\n",
        "    # replace Nan\n",
        "    df['volume'] = df['volume'].interpolate()\n",
        "    # 5 days average\n",
        "    #  moving_avg = df['close'].rolling(window=5).mean()\n",
        "    # df['moving_average'] = moving_avg\n",
        "\n",
        "    # 1) lowest and highest prices of the 5 previous days;\n",
        "    # df['high_previous_5_days'] = df['high'].shift(5)\n",
        "    # df['high_previous_4_days'] = df['high'].shift(4)\n",
        "    # df['high_previous_3_days'] = df['high'].shift(3)\n",
        "    # df['high_previous_2_days'] = df['high'].shift(2)\n",
        "    df['high_previous_1_days'] = df['high'].shift(1)\n",
        "\n",
        "    # df['low_previous_5_days'] = df['low'].shift(5)\n",
        "    # df['low_previous_4_days'] = df['low'].shift(4)\n",
        "    # df['low_previous_3_days'] = df['low'].shift(3)\n",
        "    # df['low_previous_2_days'] = df['low'].shift(2)\n",
        "    df['low_previous_1_days'] = df['low'].shift(1)\n",
        "\n",
        "\n",
        "    # 2) opening and closing prices of the 5 previous days;\n",
        "    # df['Close_previous_5_days'] = df['close'].shift(5)\n",
        "    # df['Close_previous_4_days'] = df['close'].shift(4)\n",
        "    # df['Close_previous_3_days'] = df['close'].shift(3)\n",
        "    # df['Close_previous_2_days'] = df['close'].shift(2)\n",
        "    df['close_previous_1_days'] = df['close'].shift(1)\n",
        "\n",
        "    # df['open_previous_5_days'] = df['open'].shift(5)\n",
        "    # df['open_previous_4_days'] = df['open'].shift(4)\n",
        "    # df['open_previous_3_days'] = df['open'].shift(3)\n",
        "    # df['open_previous_2_days'] = df['open'].shift(2)\n",
        "    df['open_previous_1_days'] = df['open'].shift(1)\n",
        "\n",
        "    # 3) EMA of the lowest and highest prices of the 5 previous days;\n",
        "    # 4) EMA of the opening and closing prices of the 5 previous days;\n",
        "    # 5) BB of the opening and closing prices of the 5 previous days;\n",
        "    # 6) BB of the lowest and highest prices of the 5 previous days;\n",
        "    # df[\"EMA\"] = abstract.EMA(df, 5)\n",
        "    # df[\"BB_upper\"], df[\"BB_middle\"], df[\"BB_lower\"]  = talib.BBANDS(df[\"close\"],5,matype = talib.MA_Type.EMA)\n",
        "\n",
        "    # change = today-yesterday\n",
        "    df['change'] = df['close'].diff()\n",
        "\n",
        "    # exclude 5 columns, others = input features, start from index=5 drop NaN\n",
        "    x = df[df.columns.difference(['close', 'open', 'high', 'low'])][1:]\n",
        "    y = df[\"close\"][1:]\n",
        "\n",
        "    if show_fig:\n",
        "        plt.title(\"Gold prices\")\n",
        "        plt.plot(y[:30], color=\"green\")\n",
        "        plt.plot(x.drop(columns=['volume','change'])[:30], color=\"orange\")\n",
        "        plt.xlabel(\"Days\")\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel(\"Close price\")\n",
        "        plt.show(block=False)\n",
        "        # show 5 seconds, no need in colab\n",
        "        # plt.pause(5)\n",
        "    # 2d-array x.shape = (input_samples, features)\n",
        "    x_train, x_test = to_ndarray(x, split = 0.8,features = x.shape[1])\n",
        "    y_train, y_test = to_ndarray(y, split = 0.8)\n",
        "    res = [x_train, y_train, x_test, y_test]\n",
        "    return res\n",
        "\n",
        "\n",
        "def to_ndarray(df: Union[pd.DataFrame, pd.Series], split: float = 0.8, features: int = 1):\n",
        "    nd = df.to_numpy()\n",
        "    train_set, test_set = np.split(nd, [int(split * len(nd))])\n",
        "    train_set = train_set.reshape(-1, features)\n",
        "    test_set = test_set.reshape(-1, features)\n",
        "    train_set = sc.fit_transform(train_set)\n",
        "    test_set = sc.fit_transform(test_set)\n",
        "    return train_set, test_set\n",
        "\n",
        "def drift_detection(data_stream: list, warning: float = 2.0, drift_detect: float = 3.0) -> None:\n",
        "    '''\n",
        "        prediction: int (either 0 or 1)\n",
        "            This parameter indicates whether the last sample analyzed was\n",
        "            correctly classified or not. 1 indicates an error (miss-classification).\n",
        "    '''\n",
        "    ddm = DDM(min_num_instances=30, warning_level=warning, out_control_level=drift_detect)\n",
        "    for i in range(len(data_stream)):\n",
        "        ddm.add_element(data_stream[i])\n",
        "        if ddm.detected_warning_zone():\n",
        "            pass\n",
        "            # print('Warning zone has been detected in data: ' + str(data_stream[i]) + ' - of index: ' + str(i))\n",
        "        if ddm.detected_change():\n",
        "            print('Change has been detected in data: ' + str(data_stream[i]) + ' - of index: ' + str(i))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(datasets: list[np.ndarray],nodes: int = 20, show_fig: bool = False) -> list:\n",
        "    # option: random_state=1\n",
        "    elmr = ELMRegressor(n_hidden=nodes,)\n",
        "    x_train, y_train, x_test, y_test = (\n",
        "        datasets[0],\n",
        "        datasets[1],\n",
        "        datasets[2],\n",
        "        datasets[3],\n",
        "    )\n",
        "    elmr = elmr.fit(x_train, y_train)\n",
        "    # Inverse Transform the Predictions(real world value)\n",
        "    res = elmr.predict(x_test)\n",
        "\n",
        "    y_pred = sc.inverse_transform(elmr.predict(x_test))\n",
        "    y_actual = sc.inverse_transform(y_test)\n",
        "\n",
        "    # threshold 1: compare with previous price difference\n",
        "    y_prev = np.insert(y_actual, 0, y_actual[0], axis=0)\n",
        "    y_prev = y_prev[:-1]\n",
        "    abs_diff = abs(y_actual - y_pred)\n",
        "    diff = y_actual - y_pred\n",
        "\n",
        "    prev_abs_diff = abs(y_prev - y_actual)\n",
        "    bin = abs_diff - prev_abs_diff\n",
        "    # if y prediction errors > naive errors, prediction errors, return 1\n",
        "    error_rates = np.where(bin>0, 1, 0)\n",
        "    error_rates = list(error_rates)\n",
        "    count_errors = []\n",
        "    '''\n",
        "        concept change detection method based on the PAC learning model premise,\n",
        "        that the learner’s error rate will decrease as the number of analysed samples increase,\n",
        "        as long as the data distribution is stationary.\n",
        "    '''\n",
        "    for index in range(len(error_rates)):\n",
        "        err_num = error_rates[:index].count(1) / (index+1)\n",
        "        count_errors.append(err_num)\n",
        "\n",
        "    # threshold 2: compare with price going up or down predicitons\n",
        "    y_trend = np.subtract(y_actual, y_prev)\n",
        "    trend = np.where(y_trend>0, 1, 0)\n",
        "    y_prev_pred = np.insert(y_pred, 0, y_actual[0], axis=0)\n",
        "    y_prev_pred = y_prev_pred[:-1]\n",
        "    y_pred_trend = np.subtract(y_pred, y_prev_pred)\n",
        "    pred_trend = np.where(y_pred_trend>0, 1, 0)\n",
        "    print(np.unique(trend, return_counts=True))\n",
        "    print(\"---------------------------------\")\n",
        "    print(np.unique(pred_trend, return_counts=True))\n",
        "    trend_result = list()\n",
        "    for index in range(len(pred_trend)):\n",
        "        if pred_trend[index] == trend[index]:\n",
        "            trend_result.append(0)\n",
        "        else:\n",
        "            trend_result.append(1)\n",
        "    count_trend_errors = []\n",
        "    '''\n",
        "        concept change detection method based on the PAC learning model premise,\n",
        "        that the learner’s error rate will decrease as the number of analysed samples increase,\n",
        "        as long as the data distribution is stationary.\n",
        "    '''\n",
        "    for index in range(len(trend_result)):\n",
        "        err_num = trend_result[:index].count(1) / (index+1)\n",
        "        count_trend_errors.append(err_num)\n",
        "\n",
        "\n",
        "    # errors frequency\n",
        "    stat = np.array(np.unique(diff.astype(int), return_counts=True)).T\n",
        "    errors = list()\n",
        "    freq = list()\n",
        "    for pair in stat:\n",
        "        errors.append(pair[0])\n",
        "        freq.append(pair[1])\n",
        "    size = [i*100 for i in freq]\n",
        "    if show_fig:\n",
        "        plt.clf()\n",
        "        plt.plot(range(len(y_pred-y_actual)), (y_pred-y_actual), color=\"green\")\n",
        "        plt.xlabel(\"Index\")\n",
        "        plt.ylabel(\"Errors\")\n",
        "        plt.show(block=False)\n",
        "        plt.clf()\n",
        "        plt.plot(range(len(error_rates[:200])), count_errors[:200], color=\"green\")\n",
        "        plt.xlabel(\"Number of examples\")\n",
        "        plt.ylabel(\"Errors Rates\")\n",
        "        plt.show(block=False)\n",
        "        plt.clf()\n",
        "        plt.plot(range(len(count_trend_errors[:200])), count_trend_errors[:200], color=\"blue\")\n",
        "        plt.xlabel(\"Number of examples\")\n",
        "        plt.ylabel(\"Errors Rates\")\n",
        "        plt.show(block=False)\n",
        "        plt.clf()\n",
        "        plt.scatter(errors, freq, c=\"#d62728\", s=size, alpha=0.3,)\n",
        "        plt.xlabel(\"Errors\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show(block=False)\n",
        "        # plt.pause(5)\n",
        "    # threshold 1 result & threshold 2 result\n",
        "    return [error_rates, trend_result, y_pred, y_actual]"
      ],
      "metadata": {
        "id": "aZ-Ho7uzDvz0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    @inproceedings{cavalcante2015approach,\n",
        "    title={An approach to handle concept drift in financial time series based on Extreme Learning Machines and explicit Drift Detection},\n",
        "    author={Cavalcante, Rodolfo C and Oliveira, Adriano LI},\n",
        "    booktitle={2015 international joint conference on neural networks (IJCNN)},\n",
        "    pages={1--8},\n",
        "    year={2015},\n",
        "    organization={IEEE}\n",
        "    }\n",
        "'''\n",
        "'''\n",
        "    @article{liu2019meta,\n",
        "    title={Meta-cognitive recurrent recursive kernel OS-ELM for concept drift handling},\n",
        "    author={Liu, Zongying and Loo, Chu Kiong and Seera, Manjeevan},\n",
        "    journal={Applied Soft Computing},\n",
        "    volume={75},\n",
        "    pages={494--507},\n",
        "    year={2019},\n",
        "    publisher={Elsevier}\n",
        "    }\n",
        "'''\n",
        "\n",
        "def handle_concept_drift(pred: np.ndarray, actual: np.ndarray):\n",
        "    errors = pred - actual\n",
        "\n",
        "    # test if follow normal distribution\n",
        "    # If the p-value is “small” - low probability of sampling data from a normally distributed population\n",
        "    hypo = stats.normaltest(errors)\n",
        "    print(hypo)\n",
        "    # Shapiro-Wilk test\n",
        "    stat, p_value = stats.shapiro(errors)\n",
        "    print(f\"stat: {stat} p_value: {p_value}\")\n",
        "    # Quantile-Quantile plot\n",
        "    sm.qqplot(errors, line='s')\n",
        "    plt.show()\n",
        "\n",
        "    # mean_std 可能為負 一但errors為負 則高機率出現drift detection 因此這裡轉換上存在問題\n",
        "    mean_min = 0\n",
        "    std_min = sys.maxsize\n",
        "    '''\n",
        "    method 1:\n",
        "        Both DDM and ECDD were originally proposed to identify concept drift in classification problems.\n",
        "        So, to work with a regression problem, we assumed the prediction errors as a set of values from a Normal distribution,\n",
        "        instead of a Binomial distribution as originally proposed.\n",
        "    '''\n",
        "    print(\"----------------hypothesis 1----------------\")\n",
        "    for index in range(1,len(errors)):\n",
        "        mean_t = np.mean(errors[:index])\n",
        "        std_t = np.std(errors[:index])\n",
        "        # print(f\"mean at time t {mean_t}, std at time t {std_t}\")\n",
        "        if mean_t<mean_min:\n",
        "            mean_min = mean_t\n",
        "        if std_t<std_min:\n",
        "            std_min = std_t\n",
        "        if mean_t + std_t > mean_min + 3 * std_min:\n",
        "            print(f\"drift detect at index {index}\")\n",
        "        elif mean_t + std_t > mean_min + 2 * std_min:\n",
        "            print(f\"warning level at index {index}\")\n",
        "    '''\n",
        "        In a sufficient large number of examples,\n",
        "        the binomial distribution is closely approximated by a normal distribution with the same mean and variance,\n",
        "        which means that the distribution is stationary.\n",
        "    '''\n",
        "    print(\"----------------hypothesis 2----------------\")\n",
        "    # 問題：比例問題 預測的mean落在0.01以下時 高機率會偵測到 concept drift\n",
        "    mean_min = sys.maxsize\n",
        "    std_min = sys.maxsize\n",
        "    for index in range(0,len(pred)):\n",
        "        mean_t = abs(actual[index] - pred[index])/actual[index]\n",
        "        mean_t = mean_t[0]\n",
        "        print(f\"actual {actual[index]}, pred {pred[index]}, mean {mean_t}\")\n",
        "        std_t = math.sqrt(mean_t*(1-mean_t)/(index+1))\n",
        "        # print(f\"mean at time t {mean_t}, std at time t {std_t}, mean min={mean_min}, std min={std_min}\")\n",
        "        if mean_t<mean_min:\n",
        "            mean_min = mean_t\n",
        "        if std_t<std_min:\n",
        "            std_min = std_t\n",
        "        if mean_t + std_t > mean_min + 3 * std_min:\n",
        "            print(f\"drift detect at index {index}\")\n",
        "        elif mean_t + std_t > mean_min + 2 * std_min:\n",
        "            print(f\"warning level at index {index}\")"
      ],
      "metadata": {
        "id": "HAEDoXlOXkrr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    datasets = handle_datasets(show_fig=False)\n",
        "    res = main(datasets, nodes=40, show_fig=False)\n",
        "    print(\"-------------threshold 1---------------\")\n",
        "    drift_detection(res[0], warning=1.0, drift_detect=2.0)\n",
        "    print(\"-------------threshold 2---------------\")\n",
        "    drift_detection(res[1], warning=1.0, drift_detect=2.0)\n",
        "    print(\"---------------------------------------\")\n",
        "    handle_concept_drift(res[2],res[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vZy7bywzGFkn",
        "outputId": "e9fa855b-4a4d-43cb-cdea-3ab4ee92a1b9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([0, 1]), array([247, 261]))\n",
            "---------------------------------\n",
            "(array([0, 1]), array([257, 251]))\n",
            "-------------threshold 1---------------\n",
            "Change has been detected in data: [1] - of index: 74\n",
            "Change has been detected in data: [1] - of index: 234\n",
            "Change has been detected in data: [1] - of index: 279\n",
            "-------------threshold 2---------------\n",
            "Change has been detected in data: 1 - of index: 83\n",
            "Change has been detected in data: 1 - of index: 301\n",
            "---------------------------------------\n",
            "NormaltestResult(statistic=array([553.96741746]), pvalue=array([5.09921967e-121]))\n",
            "stat: 0.40327227115631104 p_value: 9.488395930999742e-38\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------hypothesis 1----------------\n",
            "drift detect at index 2\n",
            "drift detect at index 3\n",
            "drift detect at index 4\n",
            "drift detect at index 5\n",
            "drift detect at index 6\n",
            "drift detect at index 7\n",
            "drift detect at index 8\n",
            "drift detect at index 9\n",
            "drift detect at index 10\n",
            "drift detect at index 11\n",
            "drift detect at index 12\n",
            "drift detect at index 13\n",
            "drift detect at index 14\n",
            "drift detect at index 15\n",
            "drift detect at index 16\n",
            "drift detect at index 17\n",
            "drift detect at index 18\n",
            "drift detect at index 19\n",
            "drift detect at index 20\n",
            "drift detect at index 21\n",
            "drift detect at index 22\n",
            "drift detect at index 23\n",
            "drift detect at index 24\n",
            "drift detect at index 25\n",
            "drift detect at index 26\n",
            "drift detect at index 27\n",
            "drift detect at index 28\n",
            "drift detect at index 29\n",
            "drift detect at index 30\n",
            "drift detect at index 31\n",
            "drift detect at index 32\n",
            "drift detect at index 33\n",
            "drift detect at index 34\n",
            "drift detect at index 35\n",
            "drift detect at index 36\n",
            "drift detect at index 37\n",
            "drift detect at index 38\n",
            "drift detect at index 39\n",
            "drift detect at index 40\n",
            "drift detect at index 41\n",
            "drift detect at index 42\n",
            "drift detect at index 43\n",
            "drift detect at index 44\n",
            "drift detect at index 45\n",
            "drift detect at index 46\n",
            "drift detect at index 47\n",
            "drift detect at index 48\n",
            "drift detect at index 49\n",
            "drift detect at index 50\n",
            "drift detect at index 51\n",
            "drift detect at index 52\n",
            "drift detect at index 53\n",
            "drift detect at index 54\n",
            "drift detect at index 55\n",
            "drift detect at index 56\n",
            "drift detect at index 57\n",
            "drift detect at index 58\n",
            "drift detect at index 59\n",
            "drift detect at index 60\n",
            "drift detect at index 61\n",
            "drift detect at index 62\n",
            "drift detect at index 63\n",
            "drift detect at index 64\n",
            "drift detect at index 65\n",
            "drift detect at index 66\n",
            "drift detect at index 67\n",
            "drift detect at index 68\n",
            "drift detect at index 69\n",
            "drift detect at index 70\n",
            "drift detect at index 71\n",
            "drift detect at index 72\n",
            "drift detect at index 73\n",
            "drift detect at index 74\n",
            "drift detect at index 75\n",
            "drift detect at index 76\n",
            "drift detect at index 77\n",
            "drift detect at index 78\n",
            "drift detect at index 79\n",
            "drift detect at index 80\n",
            "drift detect at index 81\n",
            "drift detect at index 82\n",
            "drift detect at index 83\n",
            "drift detect at index 84\n",
            "drift detect at index 85\n",
            "drift detect at index 86\n",
            "drift detect at index 87\n",
            "drift detect at index 88\n",
            "drift detect at index 89\n",
            "drift detect at index 90\n",
            "drift detect at index 91\n",
            "drift detect at index 92\n",
            "drift detect at index 93\n",
            "drift detect at index 94\n",
            "drift detect at index 95\n",
            "drift detect at index 96\n",
            "drift detect at index 97\n",
            "drift detect at index 98\n",
            "drift detect at index 99\n",
            "drift detect at index 100\n",
            "drift detect at index 101\n",
            "drift detect at index 102\n",
            "drift detect at index 103\n",
            "drift detect at index 104\n",
            "drift detect at index 105\n",
            "drift detect at index 106\n",
            "drift detect at index 107\n",
            "drift detect at index 108\n",
            "drift detect at index 109\n",
            "drift detect at index 110\n",
            "drift detect at index 111\n",
            "drift detect at index 112\n",
            "drift detect at index 113\n",
            "drift detect at index 114\n",
            "drift detect at index 115\n",
            "drift detect at index 116\n",
            "drift detect at index 117\n",
            "drift detect at index 118\n",
            "drift detect at index 119\n",
            "drift detect at index 120\n",
            "drift detect at index 121\n",
            "drift detect at index 122\n",
            "drift detect at index 123\n",
            "drift detect at index 124\n",
            "drift detect at index 125\n",
            "drift detect at index 126\n",
            "drift detect at index 127\n",
            "drift detect at index 128\n",
            "drift detect at index 129\n",
            "drift detect at index 130\n",
            "drift detect at index 131\n",
            "drift detect at index 132\n",
            "drift detect at index 133\n",
            "drift detect at index 134\n",
            "drift detect at index 135\n",
            "drift detect at index 136\n",
            "drift detect at index 137\n",
            "drift detect at index 138\n",
            "drift detect at index 139\n",
            "drift detect at index 140\n",
            "drift detect at index 141\n",
            "drift detect at index 142\n",
            "drift detect at index 143\n",
            "drift detect at index 144\n",
            "drift detect at index 145\n",
            "drift detect at index 146\n",
            "drift detect at index 147\n",
            "drift detect at index 148\n",
            "drift detect at index 149\n",
            "drift detect at index 150\n",
            "drift detect at index 151\n",
            "drift detect at index 152\n",
            "drift detect at index 153\n",
            "drift detect at index 154\n",
            "drift detect at index 155\n",
            "drift detect at index 156\n",
            "drift detect at index 157\n",
            "drift detect at index 158\n",
            "drift detect at index 159\n",
            "drift detect at index 160\n",
            "drift detect at index 161\n",
            "drift detect at index 162\n",
            "drift detect at index 163\n",
            "drift detect at index 164\n",
            "drift detect at index 165\n",
            "drift detect at index 166\n",
            "drift detect at index 167\n",
            "drift detect at index 168\n",
            "drift detect at index 169\n",
            "drift detect at index 170\n",
            "drift detect at index 171\n",
            "drift detect at index 172\n",
            "drift detect at index 173\n",
            "drift detect at index 174\n",
            "drift detect at index 175\n",
            "drift detect at index 176\n",
            "drift detect at index 177\n",
            "drift detect at index 178\n",
            "drift detect at index 179\n",
            "drift detect at index 180\n",
            "drift detect at index 181\n",
            "drift detect at index 182\n",
            "drift detect at index 183\n",
            "drift detect at index 184\n",
            "drift detect at index 185\n",
            "drift detect at index 186\n",
            "drift detect at index 187\n",
            "drift detect at index 188\n",
            "drift detect at index 189\n",
            "drift detect at index 190\n",
            "drift detect at index 191\n",
            "drift detect at index 192\n",
            "drift detect at index 193\n",
            "drift detect at index 194\n",
            "drift detect at index 195\n",
            "drift detect at index 196\n",
            "drift detect at index 197\n",
            "drift detect at index 198\n",
            "drift detect at index 199\n",
            "drift detect at index 200\n",
            "drift detect at index 201\n",
            "drift detect at index 202\n",
            "drift detect at index 203\n",
            "drift detect at index 204\n",
            "drift detect at index 205\n",
            "drift detect at index 206\n",
            "drift detect at index 207\n",
            "drift detect at index 208\n",
            "drift detect at index 209\n",
            "drift detect at index 210\n",
            "drift detect at index 211\n",
            "drift detect at index 212\n",
            "drift detect at index 213\n",
            "drift detect at index 214\n",
            "drift detect at index 215\n",
            "drift detect at index 216\n",
            "drift detect at index 217\n",
            "drift detect at index 218\n",
            "drift detect at index 219\n",
            "drift detect at index 220\n",
            "drift detect at index 221\n",
            "drift detect at index 222\n",
            "drift detect at index 223\n",
            "drift detect at index 224\n",
            "drift detect at index 225\n",
            "drift detect at index 226\n",
            "drift detect at index 227\n",
            "drift detect at index 228\n",
            "drift detect at index 229\n",
            "drift detect at index 230\n",
            "drift detect at index 231\n",
            "drift detect at index 232\n",
            "drift detect at index 233\n",
            "drift detect at index 234\n",
            "drift detect at index 235\n",
            "drift detect at index 236\n",
            "drift detect at index 237\n",
            "drift detect at index 238\n",
            "drift detect at index 239\n",
            "drift detect at index 240\n",
            "drift detect at index 241\n",
            "drift detect at index 242\n",
            "drift detect at index 243\n",
            "drift detect at index 244\n",
            "drift detect at index 245\n",
            "drift detect at index 246\n",
            "drift detect at index 247\n",
            "drift detect at index 248\n",
            "drift detect at index 249\n",
            "drift detect at index 250\n",
            "drift detect at index 251\n",
            "drift detect at index 252\n",
            "drift detect at index 253\n",
            "drift detect at index 254\n",
            "drift detect at index 255\n",
            "drift detect at index 256\n",
            "drift detect at index 257\n",
            "drift detect at index 258\n",
            "drift detect at index 259\n",
            "drift detect at index 260\n",
            "drift detect at index 261\n",
            "drift detect at index 262\n",
            "drift detect at index 263\n",
            "drift detect at index 264\n",
            "drift detect at index 265\n",
            "drift detect at index 266\n",
            "drift detect at index 267\n",
            "drift detect at index 268\n",
            "drift detect at index 269\n",
            "drift detect at index 270\n",
            "drift detect at index 271\n",
            "drift detect at index 272\n",
            "drift detect at index 273\n",
            "drift detect at index 274\n",
            "drift detect at index 275\n",
            "drift detect at index 276\n",
            "drift detect at index 277\n",
            "drift detect at index 278\n",
            "drift detect at index 279\n",
            "drift detect at index 280\n",
            "drift detect at index 281\n",
            "drift detect at index 282\n",
            "drift detect at index 283\n",
            "drift detect at index 284\n",
            "drift detect at index 285\n",
            "drift detect at index 286\n",
            "drift detect at index 287\n",
            "drift detect at index 288\n",
            "drift detect at index 289\n",
            "drift detect at index 290\n",
            "drift detect at index 291\n",
            "drift detect at index 292\n",
            "drift detect at index 293\n",
            "drift detect at index 294\n",
            "drift detect at index 295\n",
            "drift detect at index 296\n",
            "drift detect at index 297\n",
            "drift detect at index 298\n",
            "drift detect at index 299\n",
            "drift detect at index 300\n",
            "drift detect at index 301\n",
            "drift detect at index 302\n",
            "drift detect at index 303\n",
            "drift detect at index 304\n",
            "drift detect at index 305\n",
            "drift detect at index 306\n",
            "drift detect at index 307\n",
            "drift detect at index 308\n",
            "drift detect at index 309\n",
            "drift detect at index 310\n",
            "drift detect at index 311\n",
            "drift detect at index 312\n",
            "drift detect at index 313\n",
            "drift detect at index 314\n",
            "drift detect at index 315\n",
            "drift detect at index 316\n",
            "drift detect at index 317\n",
            "drift detect at index 318\n",
            "drift detect at index 319\n",
            "drift detect at index 320\n",
            "drift detect at index 321\n",
            "drift detect at index 322\n",
            "drift detect at index 323\n",
            "drift detect at index 324\n",
            "drift detect at index 325\n",
            "drift detect at index 326\n",
            "drift detect at index 327\n",
            "drift detect at index 328\n",
            "drift detect at index 329\n",
            "drift detect at index 330\n",
            "drift detect at index 331\n",
            "drift detect at index 332\n",
            "drift detect at index 333\n",
            "drift detect at index 334\n",
            "drift detect at index 335\n",
            "drift detect at index 336\n",
            "drift detect at index 337\n",
            "drift detect at index 338\n",
            "drift detect at index 339\n",
            "drift detect at index 340\n",
            "drift detect at index 341\n",
            "drift detect at index 342\n",
            "drift detect at index 343\n",
            "drift detect at index 344\n",
            "drift detect at index 345\n",
            "drift detect at index 346\n",
            "drift detect at index 347\n",
            "drift detect at index 348\n",
            "drift detect at index 349\n",
            "drift detect at index 350\n",
            "drift detect at index 351\n",
            "drift detect at index 352\n",
            "drift detect at index 353\n",
            "drift detect at index 354\n",
            "drift detect at index 355\n",
            "drift detect at index 356\n",
            "drift detect at index 357\n",
            "drift detect at index 358\n",
            "drift detect at index 359\n",
            "drift detect at index 360\n",
            "drift detect at index 361\n",
            "drift detect at index 362\n",
            "drift detect at index 363\n",
            "drift detect at index 364\n",
            "drift detect at index 365\n",
            "drift detect at index 366\n",
            "drift detect at index 367\n",
            "drift detect at index 368\n",
            "drift detect at index 369\n",
            "drift detect at index 370\n",
            "drift detect at index 371\n",
            "drift detect at index 372\n",
            "drift detect at index 373\n",
            "drift detect at index 374\n",
            "drift detect at index 375\n",
            "drift detect at index 376\n",
            "drift detect at index 377\n",
            "drift detect at index 378\n",
            "drift detect at index 379\n",
            "drift detect at index 380\n",
            "drift detect at index 381\n",
            "drift detect at index 382\n",
            "drift detect at index 383\n",
            "drift detect at index 384\n",
            "drift detect at index 385\n",
            "drift detect at index 386\n",
            "drift detect at index 387\n",
            "drift detect at index 388\n",
            "drift detect at index 389\n",
            "drift detect at index 390\n",
            "drift detect at index 391\n",
            "drift detect at index 392\n",
            "drift detect at index 393\n",
            "drift detect at index 394\n",
            "drift detect at index 395\n",
            "drift detect at index 396\n",
            "drift detect at index 397\n",
            "drift detect at index 398\n",
            "drift detect at index 399\n",
            "drift detect at index 400\n",
            "drift detect at index 401\n",
            "drift detect at index 402\n",
            "drift detect at index 403\n",
            "drift detect at index 404\n",
            "drift detect at index 405\n",
            "drift detect at index 406\n",
            "drift detect at index 407\n",
            "drift detect at index 408\n",
            "drift detect at index 409\n",
            "drift detect at index 410\n",
            "drift detect at index 411\n",
            "drift detect at index 412\n",
            "drift detect at index 413\n",
            "drift detect at index 414\n",
            "drift detect at index 415\n",
            "drift detect at index 416\n",
            "drift detect at index 417\n",
            "drift detect at index 418\n",
            "drift detect at index 419\n",
            "drift detect at index 420\n",
            "drift detect at index 421\n",
            "drift detect at index 422\n",
            "drift detect at index 423\n",
            "drift detect at index 424\n",
            "drift detect at index 425\n",
            "drift detect at index 426\n",
            "drift detect at index 427\n",
            "drift detect at index 428\n",
            "drift detect at index 429\n",
            "drift detect at index 430\n",
            "drift detect at index 431\n",
            "drift detect at index 432\n",
            "drift detect at index 433\n",
            "drift detect at index 434\n",
            "drift detect at index 435\n",
            "drift detect at index 436\n",
            "drift detect at index 437\n",
            "drift detect at index 438\n",
            "drift detect at index 439\n",
            "drift detect at index 440\n",
            "drift detect at index 441\n",
            "drift detect at index 442\n",
            "drift detect at index 443\n",
            "drift detect at index 444\n",
            "drift detect at index 445\n",
            "drift detect at index 446\n",
            "drift detect at index 447\n",
            "drift detect at index 448\n",
            "drift detect at index 449\n",
            "drift detect at index 450\n",
            "drift detect at index 451\n",
            "drift detect at index 452\n",
            "drift detect at index 453\n",
            "drift detect at index 454\n",
            "drift detect at index 455\n",
            "drift detect at index 456\n",
            "drift detect at index 457\n",
            "drift detect at index 458\n",
            "drift detect at index 459\n",
            "drift detect at index 460\n",
            "drift detect at index 461\n",
            "drift detect at index 462\n",
            "drift detect at index 463\n",
            "drift detect at index 464\n",
            "drift detect at index 465\n",
            "drift detect at index 466\n",
            "drift detect at index 467\n",
            "drift detect at index 468\n",
            "drift detect at index 469\n",
            "drift detect at index 470\n",
            "drift detect at index 471\n",
            "drift detect at index 472\n",
            "drift detect at index 473\n",
            "drift detect at index 474\n",
            "drift detect at index 475\n",
            "drift detect at index 476\n",
            "drift detect at index 477\n",
            "drift detect at index 478\n",
            "drift detect at index 479\n",
            "drift detect at index 480\n",
            "drift detect at index 481\n",
            "drift detect at index 482\n",
            "drift detect at index 483\n",
            "drift detect at index 484\n",
            "drift detect at index 485\n",
            "drift detect at index 486\n",
            "drift detect at index 487\n",
            "drift detect at index 488\n",
            "drift detect at index 489\n",
            "drift detect at index 490\n",
            "drift detect at index 491\n",
            "drift detect at index 492\n",
            "drift detect at index 493\n",
            "drift detect at index 494\n",
            "drift detect at index 495\n",
            "drift detect at index 496\n",
            "drift detect at index 497\n",
            "drift detect at index 498\n",
            "drift detect at index 499\n",
            "drift detect at index 500\n",
            "drift detect at index 501\n",
            "drift detect at index 502\n",
            "drift detect at index 503\n",
            "drift detect at index 504\n",
            "drift detect at index 505\n",
            "drift detect at index 506\n",
            "drift detect at index 507\n",
            "----------------hypothesis 2----------------\n",
            "actual [1753.3], pred [1750.40088712], mean 0.0016535178709798613\n",
            "actual [1751.8], pred [1747.23333787], mean 0.0026068398984596093\n",
            "actual [1778.2], pred [1764.01298437], mean 0.007978301446594761\n",
            "actual [1789.8], pred [1776.98404772], mean 0.007160549936285004\n",
            "actual [1787.8], pred [1786.27137944], mean 0.000855028838859781\n",
            "actual [1784.4], pred [1786.07985859], mean 0.0009414136927450694\n",
            "actual [1783.1], pred [1784.45048912], mean 0.0007573827169127089\n",
            "actual [1784.], pred [1784.71478488], mean 0.0004006641702639143\n",
            "actual [1806.3], pred [1791.3515088], mean 0.00827575220280292\n",
            "drift detect at index 8\n",
            "actual [1808.5], pred [1803.15458214], mean 0.002955719028362587\n",
            "warning level at index 9\n",
            "actual [1791.], pred [1800.8801629], mean 0.005516562200686337\n",
            "drift detect at index 10\n",
            "actual [1795.2], pred [1795.97215559], mean 0.0004301223232133035\n",
            "actual [1819.5], pred [1802.05814566], mean 0.00958606998534245\n",
            "drift detect at index 12\n",
            "actual [1812.2], pred [1814.13720182], mean 0.0010689779369956716\n",
            "actual [1818.1], pred [1816.1146002], mean 0.0010920190324856128\n",
            "actual [1816.], pred [1815.52100705], mean 0.00026376263987509097\n",
            "actual [1811.5], pred [1813.03941765], mean 0.0008498027313915527\n",
            "actual [1833.7], pred [1818.85194017], mean 0.008097322259514508\n",
            "drift detect at index 17\n",
            "actual [1798.5], pred [1814.11696694], mean 0.008683328851773489\n",
            "drift detect at index 18\n",
            "actual [1793.5], pred [1809.74427833], mean 0.009057306010360313\n",
            "drift detect at index 19\n",
            "actual [1800.], pred [1796.41737481], mean 0.0019903473258312313\n",
            "warning level at index 20\n",
            "actual [1792.1], pred [1794.06684611], mean 0.0010975091311845717\n",
            "actual [1794.4], pred [1793.9054927], mean 0.00027558364884812733\n",
            "actual [1807.1], pred [1796.57244066], mean 0.005825665066086357\n",
            "drift detect at index 23\n",
            "actual [1794.8], pred [1800.96719647], mean 0.0034361469072274223\n",
            "drift detect at index 24\n",
            "actual [1756.7], pred [1785.69980221], mean 0.016508113057255327\n",
            "drift detect at index 25\n",
            "actual [1751.4], pred [1765.61389352], mean 0.008115732281860384\n",
            "drift detect at index 26\n",
            "actual [1763.8], pred [1754.67460593], mean 0.005173712478917289\n",
            "drift detect at index 27\n",
            "actual [1778.2], pred [1766.152088], mean 0.006775341354379795\n",
            "drift detect at index 28\n",
            "actual [1778.8], pred [1774.88086803], mean 0.0022032448683728294\n",
            "drift detect at index 29\n",
            "actual [1749.8], pred [1765.42512232], mean 0.00892966185744217\n",
            "drift detect at index 30\n",
            "actual [1751.7], pred [1754.24027079], mean 0.0014501745662967357\n",
            "warning level at index 31\n",
            "actual [1752.], pred [1748.18070241], mean 0.0021799643769901448\n",
            "warning level at index 32\n",
            "actual [1737.5], pred [1746.62874981], mean 0.005253956727765726\n",
            "drift detect at index 33\n",
            "actual [1722.9], pred [1732.8914972], mean 0.0057992322241079385\n",
            "drift detect at index 34\n",
            "actual [1757.], pred [1743.01050418], mean 0.007962149015786393\n",
            "drift detect at index 35\n",
            "actual [1758.4], pred [1749.98650953], mean 0.00478474208010716\n",
            "drift detect at index 36\n",
            "actual [1767.6], pred [1761.34850944], mean 0.0035367111083270068\n",
            "drift detect at index 37\n",
            "actual [1760.9], pred [1763.98428457], mean 0.0017515387391781102\n",
            "warning level at index 38\n",
            "actual [1761.8], pred [1763.77043059], mean 0.00111841899919121\n",
            "actual [1759.2], pred [1760.69325797], mean 0.0008488278587331106\n",
            "actual [1757.4], pred [1762.04322508], mean 0.002642099168200943\n",
            "warning level at index 41\n",
            "actual [1755.7], pred [1754.75900416], mean 0.0005359661892827955\n",
            "actual [1759.3], pred [1758.2046735], mean 0.0006225922219871421\n",
            "actual [1794.7], pred [1771.7733346], mean 0.012774650582711943\n",
            "drift detect at index 44\n",
            "actual [1797.9], pred [1788.94503751], mean 0.004980790085654106\n",
            "drift detect at index 45\n",
            "actual [1768.3], pred [1785.52375919], mean 0.009740292479060327\n",
            "drift detect at index 46\n",
            "actual [1765.7], pred [1776.47688723], mean 0.00610346447750135\n",
            "drift detect at index 47\n",
            "actual [1770.5], pred [1767.43673838], mean 0.0017301675319504079\n",
            "warning level at index 48\n",
            "actual [1784.9], pred [1772.6533066], mean 0.0068612770463830144\n",
            "drift detect at index 49\n",
            "actual [1781.9], pred [1778.85125795], mean 0.0017109501348250785\n",
            "warning level at index 50\n",
            "actual [1796.3], pred [1779.37862058], mean 0.009420129945828961\n",
            "drift detect at index 51\n",
            "actual [1806.8], pred [1799.55202613], mean 0.00401149760306884\n",
            "drift detect at index 52\n",
            "actual [1793.4], pred [1798.01638242], mean 0.0025740952484210114\n",
            "warning level at index 53\n",
            "actual [1798.8], pred [1798.79916454], mean 4.644546179424663e-07\n",
            "actual [1802.6], pred [1799.61337631], mean 0.001656842164410534\n",
            "drift detect at index 55\n",
            "actual [1783.9], pred [1795.58146576], mean 0.006548273874574559\n",
            "drift detect at index 56\n",
            "actual [1795.8], pred [1791.32568893], mean 0.002491541969034005\n",
            "drift detect at index 57\n",
            "actual [1789.4], pred [1789.7973582], mean 0.00022206225412563558\n",
            "drift detect at index 58\n",
            "actual [1763.9], pred [1782.84477302], mean 0.010740276105936378\n",
            "drift detect at index 59\n",
            "actual [1793.5], pred [1772.43381134], mean 0.011745853725948937\n",
            "drift detect at index 60\n",
            "actual [1816.8], pred [1787.3353279], mean 0.016217895256735042\n",
            "drift detect at index 61\n",
            "actual [1828.], pred [1816.33539708], mean 0.006381073806713855\n",
            "drift detect at index 62\n",
            "actual [1830.8], pred [1826.32935481], mean 0.0024419080132153832\n",
            "drift detect at index 63\n",
            "actual [1848.3], pred [1830.2675385], mean 0.009756241679216945\n",
            "drift detect at index 64\n",
            "actual [1863.9], pred [1854.90561637], mean 0.004825571987548818\n",
            "drift detect at index 65\n",
            "actual [1868.5], pred [1862.30488825], mean 0.0033155535192681993\n",
            "drift detect at index 66\n",
            "actual [1866.6], pred [1862.81635709], mean 0.002027023952893939\n",
            "drift detect at index 67\n",
            "actual [1854.1], pred [1857.17492061], mean 0.0016584437803729753\n",
            "drift detect at index 68\n",
            "actual [1872.8], pred [1860.92723638], mean 0.006339579037730591\n",
            "drift detect at index 69\n",
            "actual [1864.], pred [1864.23612853], mean 0.00012667839394364238\n",
            "drift detect at index 70\n",
            "actual [1854.3], pred [1857.29629731], mean 0.0016158643767712779\n",
            "drift detect at index 71\n",
            "actual [1809.1], pred [1857.66536896], mean 0.026845043921872588\n",
            "drift detect at index 72\n",
            "actual [1786.3], pred [1822.59145488], mean 0.020316550902714208\n",
            "drift detect at index 73\n",
            "actual [1786.9], pred [1794.7341072], mean 0.004384188931941444\n",
            "drift detect at index 74\n",
            "actual [1788.1], pred [1788.7566719], mean 0.0003672456247391954\n",
            "drift detect at index 75\n",
            "actual [1785.2], pred [1792.3890736], mean 0.004027041006594351\n",
            "drift detect at index 76\n",
            "actual [1776.5], pred [1785.39420427], mean 0.005006588389835616\n",
            "drift detect at index 77\n",
            "actual [1784.3], pred [1782.37813857], mean 0.0010770954624412495\n",
            "drift detect at index 78\n",
            "actual [1762.7], pred [1771.23395615], mean 0.004841411556890459\n",
            "drift detect at index 79\n",
            "actual [1783.9], pred [1771.3954635], mean 0.0070096622547595\n",
            "drift detect at index 80\n",
            "actual [1779.5], pred [1777.38148187], mean 0.0011905131375790762\n",
            "drift detect at index 81\n",
            "actual [1784.7], pred [1782.57232451], mean 0.0011921754312182718\n",
            "drift detect at index 82\n",
            "actual [1785.5], pred [1782.40776265], mean 0.001731860740294994\n",
            "drift detect at index 83\n",
            "actual [1776.7], pred [1780.26434524], mean 0.002006160434362754\n",
            "drift detect at index 84\n",
            "actual [1784.8], pred [1780.71867889], mean 0.0022867106181597785\n",
            "drift detect at index 85\n",
            "actual [1788.3], pred [1783.44483072], mean 0.0027149635276872866\n",
            "drift detect at index 86\n",
            "actual [1772.3], pred [1778.30596745], mean 0.0033887984234125834\n",
            "drift detect at index 87\n",
            "actual [1764.5], pred [1774.21773183], mean 0.00550735723076089\n",
            "drift detect at index 88\n",
            "actual [1798.2], pred [1779.14596805], mean 0.010596169474070912\n",
            "drift detect at index 89\n",
            "actual [1804.9], pred [1796.00321673], mean 0.004929238889751534\n",
            "drift detect at index 90\n",
            "actual [1794.6], pred [1800.62933804], mean 0.0033597113806497305\n",
            "drift detect at index 91\n",
            "actual [1788.7], pred [1793.49478893], mean 0.0026805998394017452\n",
            "drift detect at index 92\n",
            "actual [1802.2], pred [1794.38463291], mean 0.00433657035224942\n",
            "drift detect at index 93\n",
            "actual [1811.7], pred [1803.20130222], mean 0.0046910072183280515\n",
            "drift detect at index 94\n",
            "actual [1808.8], pred [1806.10537462], mean 0.0014897309682340388\n",
            "drift detect at index 95\n",
            "actual [1810.9], pred [1807.91637149], mean 0.001647594297168338\n",
            "drift detect at index 96\n",
            "actual [1805.8], pred [1809.25940542], mean 0.0019157190281602447\n",
            "drift detect at index 97\n",
            "actual [1814.1], pred [1807.24199258], mean 0.0037803910565982384\n",
            "drift detect at index 98\n",
            "actual [1828.6], pred [1818.38032018], mean 0.005588800076190892\n",
            "drift detect at index 99\n",
            "actual [1800.1], pred [1817.18012062], mean 0.009488428763298414\n",
            "drift detect at index 100\n",
            "actual [1814.6], pred [1810.23017094], mean 0.002408150037819435\n",
            "drift detect at index 101\n",
            "actual [1825.1], pred [1815.15629143], mean 0.005448308897696012\n",
            "drift detect at index 102\n",
            "actual [1789.2], pred [1811.92061427], mean 0.01269875601666222\n",
            "drift detect at index 103\n",
            "actual [1797.4], pred [1794.60904648], mean 0.0015527726259749592\n",
            "drift detect at index 104\n",
            "actual [1798.8], pred [1795.89317689], mean 0.0016159790487018492\n",
            "drift detect at index 105\n",
            "actual [1818.5], pred [1804.23072127], mean 0.007846730122899872\n",
            "drift detect at index 106\n",
            "actual [1827.3], pred [1818.22454209], mean 0.0049665943792976865\n",
            "drift detect at index 107\n",
            "actual [1821.4], pred [1822.95102465], mean 0.0008515563032404653\n",
            "drift detect at index 108\n",
            "actual [1816.5], pred [1820.06035831], mean 0.001960010082521575\n",
            "drift detect at index 109\n",
            "actual [1812.4], pred [1822.87592539], mean 0.005780139808430853\n",
            "drift detect at index 110\n",
            "actual [1845.5], pred [1827.08649575], mean 0.009977515171840114\n",
            "drift detect at index 111\n",
            "actual [1844.9], pred [1843.02082768], mean 0.0010185767918580503\n",
            "drift detect at index 112\n",
            "actual [1834.1], pred [1837.29026328], mean 0.001739416215228485\n",
            "drift detect at index 113\n",
            "actual [1844.2], pred [1838.56496642], mean 0.0030555436401225084\n",
            "drift detect at index 114\n",
            "actual [1855.], pred [1846.35858256], mean 0.004658446061956797\n",
            "drift detect at index 115\n",
            "actual [1832.], pred [1844.03427321], mean 0.006568926426421128\n",
            "drift detect at index 116\n",
            "actual [1795.], pred [1836.69239833], mean 0.02322696286018779\n",
            "drift detect at index 117\n",
            "actual [1786.6], pred [1802.72708679], mean 0.009026691365956123\n",
            "drift detect at index 118\n",
            "actual [1796.4], pred [1792.61430321], mean 0.00210737964001067\n",
            "drift detect at index 119\n",
            "actual [1801.5], pred [1796.65245597], mean 0.0026908376514871222\n",
            "drift detect at index 120\n",
            "actual [1810.3], pred [1803.8655427], mean 0.003554359666415834\n",
            "drift detect at index 121\n",
            "actual [1804.1], pred [1805.46454366], mean 0.0007563569952270742\n",
            "drift detect at index 122\n",
            "actual [1807.8], pred [1805.92934948], mean 0.001034766303798432\n",
            "drift detect at index 123\n",
            "actual [1821.8], pred [1812.35447604], mean 0.005184720585882011\n",
            "drift detect at index 124\n",
            "actual [1827.9], pred [1821.42840011], mean 0.0035404562037299188\n",
            "drift detect at index 125\n",
            "actual [1836.6], pred [1829.44577328], mean 0.003895364654575025\n",
            "drift detect at index 126\n",
            "actual [1837.4], pred [1833.47445568], mean 0.0021364669230473253\n",
            "drift detect at index 127\n",
            "actual [1842.1], pred [1837.86719659], mean 0.002297814128315525\n",
            "drift detect at index 128\n",
            "actual [1869.4], pred [1853.58528387], mean 0.008459781818604252\n",
            "drift detect at index 129\n",
            "actual [1856.2], pred [1858.27560411], mean 0.0011182006836395187\n",
            "drift detect at index 130\n",
            "actual [1871.5], pred [1863.68021308], mean 0.004178352618094092\n",
            "drift detect at index 131\n",
            "actual [1902.], pred [1888.87599234], mean 0.006900109181219776\n",
            "drift detect at index 132\n",
            "actual [1899.8], pred [1895.34495579], mean 0.002345006952228011\n",
            "drift detect at index 133\n",
            "actual [1907.4], pred [1902.93695674], mean 0.0023398570110443506\n",
            "drift detect at index 134\n",
            "actual [1910.4], pred [1903.64975435], mean 0.0035334200429151578\n",
            "drift detect at index 135\n",
            "actual [1926.3], pred [1917.33869106], mean 0.004652083757582633\n",
            "drift detect at index 136\n",
            "actual [1887.6], pred [1914.99270593], mean 0.014511923037477238\n",
            "drift detect at index 137\n",
            "actual [1900.7], pred [1891.72530711], mean 0.0047217829677143166\n",
            "drift detect at index 138\n",
            "actual [1943.8], pred [1913.92058552], mean 0.015371650620255915\n",
            "drift detect at index 139\n",
            "actual [1922.3], pred [1925.35519455], mean 0.0015893432613601024\n",
            "drift detect at index 140\n",
            "actual [1935.9], pred [1926.73869427], mean 0.0047323238466621936\n",
            "drift detect at index 141\n",
            "actual [1966.6], pred [1953.31620747], mean 0.0067546997531234\n",
            "drift detect at index 142\n",
            "actual [1995.9], pred [1998.68684144], mean 0.0013962831022994331\n",
            "drift detect at index 143\n",
            "actual [2043.3], pred [2050.21643919], mean 0.0033849357347257116\n",
            "drift detect at index 144\n",
            "actual [1988.2], pred [1996.78380998], mean 0.004317377515842154\n",
            "drift detect at index 145\n",
            "actual [2000.4], pred [1984.53277729], mean 0.007932024949507642\n",
            "drift detect at index 146\n",
            "actual [1985.], pred [1997.90519275], mean 0.006501356549077937\n",
            "drift detect at index 147\n",
            "actual [1960.8], pred [1971.19582468], mean 0.005301828171065021\n",
            "drift detect at index 148\n",
            "actual [1929.7], pred [1958.01898942], mean 0.014675332654747377\n",
            "drift detect at index 149\n",
            "actual [1909.2], pred [1913.45683338], mean 0.0022296424560525442\n",
            "drift detect at index 150\n",
            "actual [1948.2], pred [1906.8087719], mean 0.02124588240309921\n",
            "drift detect at index 151\n",
            "actual [1933.9], pred [1939.40146669], mean 0.0028447524100893113\n",
            "drift detect at index 152\n",
            "actual [1934.8], pred [1935.04614919], mean 0.00012722203372630622\n",
            "drift detect at index 153\n",
            "actual [1926.7], pred [1929.7424023], mean 0.0015790742211217012\n",
            "drift detect at index 154\n",
            "actual [1942.6], pred [1930.74929662], mean 0.006100434151117932\n",
            "drift detect at index 155\n",
            "actual [1967.7], pred [1941.83282823], mean 0.013145892040886842\n",
            "drift detect at index 156\n",
            "actual [1959.8], pred [1957.54651524], mean 0.0011498544570493728\n",
            "drift detect at index 157\n",
            "actual [1944.7], pred [1945.08117035], mean 0.00019600470295278867\n",
            "drift detect at index 158\n",
            "actual [1918.], pred [1932.41963987], mean 0.007518060413054524\n",
            "drift detect at index 159\n",
            "actual [1939.], pred [1919.33488756], mean 0.010141883670317733\n",
            "drift detect at index 160\n",
            "actual [1954.], pred [1941.06593818], mean 0.006619274218913521\n",
            "drift detect at index 161\n",
            "actual [1923.7], pred [1930.20472299], mean 0.0033813603934688364\n",
            "drift detect at index 162\n",
            "actual [1934.], pred [1927.00598109], mean 0.003616348969169175\n",
            "drift detect at index 163\n",
            "actual [1927.5], pred [1921.48389352], mean 0.003121196616726491\n",
            "drift detect at index 164\n",
            "actual [1923.1], pred [1918.63927912], mean 0.002319547021522607\n",
            "drift detect at index 165\n",
            "actual [1937.8], pred [1927.08648112], mean 0.005528702072748673\n",
            "drift detect at index 166\n",
            "actual [1945.6], pred [1936.48535322], mean 0.004684748549553515\n",
            "drift detect at index 167\n",
            "actual [1948.2], pred [1941.22748303], mean 0.0035789533788425053\n",
            "drift detect at index 168\n",
            "actual [1976.1], pred [1960.85299579], mean 0.007715704777953459\n",
            "drift detect at index 169\n",
            "actual [1984.7], pred [1972.59689172], mean 0.006098205408974641\n",
            "drift detect at index 170\n",
            "actual [1974.9], pred [1971.83245483], mean 0.0015532660752799472\n",
            "drift detect at index 171\n",
            "actual [1986.4], pred [1976.96915879], mean 0.004747705001371588\n",
            "drift detect at index 172\n",
            "actual [1959.], pred [1981.31473399], mean 0.011390880033639339\n",
            "drift detect at index 173\n",
            "actual [1955.6], pred [1949.35500346], mean 0.003193391563695318\n",
            "drift detect at index 174\n",
            "actual [1948.2], pred [1943.48598572], mean 0.0024196767680127198\n",
            "drift detect at index 175\n",
            "actual [1934.3], pred [1936.69742416], mean 0.001239427267867262\n",
            "drift detect at index 176\n",
            "actual [1896.], pred [1933.01023113], mean 0.019520164097250395\n",
            "drift detect at index 177\n",
            "actual [1904.1], pred [1897.10753366], mean 0.0036723209586762293\n",
            "drift detect at index 178\n",
            "actual [1888.7], pred [1887.82564559], mean 0.00046293980698723224\n",
            "drift detect at index 179\n",
            "actual [1891.3], pred [1886.48964469], mean 0.002543412105156104\n",
            "drift detect at index 180\n",
            "actual [1911.7], pred [1897.80287683], mean 0.007269510470516375\n",
            "drift detect at index 181\n",
            "actual [1863.6], pred [1903.47230682], mean 0.02139531381435155\n",
            "drift detect at index 182\n",
            "actual [1870.6], pred [1867.45305662], mean 0.0016823176390699463\n",
            "drift detect at index 183\n",
            "actual [1868.8], pred [1866.43512651], mean 0.0012654502811565058\n",
            "drift detect at index 184\n",
            "actual [1875.7], pred [1870.36308574], mean 0.002845292027724169\n",
            "drift detect at index 185\n",
            "actual [1882.8], pred [1878.5240562], mean 0.0022710557703200427\n",
            "drift detect at index 186\n",
            "actual [1858.6], pred [1868.39319211], mean 0.005269123055205418\n",
            "drift detect at index 187\n",
            "actual [1841.], pred [1857.92447182], mean 0.009193086270775124\n",
            "drift detect at index 188\n",
            "actual [1853.7], pred [1846.3981089], mean 0.003939089984564068\n",
            "drift detect at index 189\n",
            "actual [1824.6], pred [1840.67843289], mean 0.008812031618813464\n",
            "drift detect at index 190\n",
            "actual [1808.2], pred [1830.49783517], mean 0.012331509331277454\n",
            "drift detect at index 191\n",
            "actual [1814.], pred [1814.09480749], mean 5.2264328229643714e-05\n",
            "drift detect at index 192\n",
            "actual [1818.9], pred [1816.74621229], mean 0.0011841155121595957\n",
            "drift detect at index 193\n",
            "actual [1822.4], pred [1817.89341801], mean 0.002472883007765724\n",
            "drift detect at index 194\n",
            "actual [1847.8], pred [1831.23425487], mean 0.008965118047226415\n",
            "drift detect at index 195\n",
            "actual [1848.4], pred [1846.16684007], mean 0.0012081583712361642\n",
            "drift detect at index 196\n",
            "actual [1853.9], pred [1847.24957854], mean 0.0035872600761129203\n",
            "drift detect at index 197\n",
            "actual [1871.4], pred [1861.99156524], mean 0.005027484641691611\n",
            "drift detect at index 198\n",
            "actual [1852.5], pred [1860.94129552], mean 0.0045567047332475685\n",
            "drift detect at index 199\n",
            "actual [1853.9], pred [1853.93726899], mean 2.0103020885326418e-05\n",
            "drift detect at index 200\n",
            "actual [1857.3], pred [1853.35240644], mean 0.0021254474543819187\n",
            "drift detect at index 201\n",
            "actual [1848.4], pred [1849.34410889], mean 0.0005107708757827375\n",
            "drift detect at index 202\n",
            "actual [1848.7], pred [1849.653861], mean 0.0005159631067413349\n",
            "drift detect at index 203\n",
            "actual [1871.4], pred [1858.06529808], mean 0.007125522028556947\n",
            "drift detect at index 204\n",
            "actual [1850.2], pred [1861.29214218], mean 0.005995104408728993\n",
            "drift detect at index 205\n",
            "actual [1843.7], pred [1851.30315994], mean 0.00412385959534599\n",
            "drift detect at index 206\n",
            "actual [1852.1], pred [1848.48512792], mean 0.0019517693885527493\n",
            "drift detect at index 207\n",
            "actual [1856.5], pred [1852.89860791], mean 0.0019398826257636802\n",
            "drift detect at index 208\n",
            "actual [1852.8], pred [1852.38322164], mean 0.00022494514359266502\n",
            "drift detect at index 209\n",
            "actual [1875.5], pred [1864.35476254], mean 0.0059425419683296\n",
            "drift detect at index 210\n",
            "actual [1831.8], pred [1866.29050187], mean 0.018828748700736177\n",
            "drift detect at index 211\n",
            "actual [1813.5], pred [1842.8584922], mean 0.01618885701586844\n",
            "drift detect at index 212\n",
            "actual [1819.6], pred [1817.16440735], mean 0.001338531899898491\n",
            "drift detect at index 213\n",
            "actual [1849.9], pred [1831.98686126], mean 0.009683301117056837\n",
            "drift detect at index 214\n",
            "actual [1840.6], pred [1846.34015208], mean 0.003118630924487059\n",
            "drift detect at index 215\n",
            "actual [1838.8], pred [1842.26260151], mean 0.0018830767415669721\n",
            "drift detect at index 216\n",
            "actual [1838.4], pred [1837.98996408], mean 0.00022303955392383412\n",
            "drift detect at index 217\n",
            "actual [1829.8], pred [1834.80745398], mean 0.0027366127341464292\n",
            "drift detect at index 218\n",
            "actual [1830.3], pred [1828.28682591], mean 0.0010999148178117302\n",
            "drift detect at index 219\n",
            "actual [1824.8], pred [1825.79784852], mean 0.0005468262389385515\n",
            "drift detect at index 220\n",
            "actual [1821.2], pred [1826.32356769], mean 0.002813292166984947\n",
            "drift detect at index 221\n",
            "actual [1817.5], pred [1819.4642743], mean 0.0010807561488859474\n",
            "drift detect at index 222\n",
            "actual [1807.3], pred [1816.30650429], mean 0.004983403024319074\n",
            "drift detect at index 223\n",
            "actual [1801.5], pred [1810.61158716], mean 0.005057778050739394\n",
            "drift detect at index 224\n",
            "actual [1763.9], pred [1793.26916974], mean 0.016650133081036987\n",
            "drift detect at index 225\n",
            "actual [1736.5], pred [1770.6282467], mean 0.019653467720925882\n",
            "drift detect at index 226\n",
            "actual [1739.7], pred [1740.83397075], mean 0.0006518197087951968\n",
            "drift detect at index 227\n",
            "actual [1742.3], pred [1740.79144776], mean 0.0008658395478414229\n",
            "drift detect at index 228\n",
            "actual [1731.7], pred [1736.27282641], mean 0.002640657394651458\n",
            "drift detect at index 229\n",
            "actual [1724.8], pred [1741.47893775], mean 0.009670070587489898\n",
            "drift detect at index 230\n",
            "actual [1735.5], pred [1735.57408344], mean 4.268708994177624e-05\n",
            "drift detect at index 231\n",
            "actual [1705.8], pred [1746.14724648], mean 0.023652976013764832\n",
            "drift detect at index 232\n",
            "actual [1703.6], pred [1702.43867152], mean 0.0006816908215733507\n",
            "drift detect at index 233\n",
            "actual [1710.2], pred [1724.92862503], mean 0.008612223733012344\n",
            "drift detect at index 234\n",
            "actual [1728.1], pred [1727.29654767], mean 0.0004649339353981078\n",
            "drift detect at index 235\n",
            "actual [1717.7], pred [1722.88283432], mean 0.0030173105405305557\n",
            "drift detect at index 236\n",
            "actual [1731.3], pred [1731.9751437], mean 0.0003899634376451699\n",
            "drift detect at index 237\n",
            "actual [1745.3], pred [1739.51976827], mean 0.0033118843354747866\n",
            "drift detect at index 238\n",
            "actual [1737.1], pred [1737.88821251], mean 0.0004537519503414407\n",
            "drift detect at index 239\n",
            "actual [1735.7], pred [1737.08365767], mean 0.0007971755904867978\n",
            "drift detect at index 240\n",
            "actual [1737.5], pred [1735.91149636], mean 0.0009142466981368363\n",
            "drift detect at index 241\n",
            "actual [1769.2], pred [1751.91864971], mean 0.009767889606651115\n",
            "drift detect at index 242\n",
            "actual [1781.8], pred [1769.15399082], mean 0.007097322472812716\n",
            "drift detect at index 243\n",
            "actual [1787.7], pred [1780.90498557], mean 0.003800981389969313\n",
            "drift detect at index 244\n",
            "actual [1789.7], pred [1786.91565735], mean 0.0015557594277160601\n",
            "drift detect at index 245\n",
            "actual [1776.4], pred [1786.30755912], mean 0.005577324432131866\n",
            "drift detect at index 246\n",
            "actual [1806.9], pred [1788.98840142], mean 0.009912888692960104\n",
            "drift detect at index 247\n",
            "actual [1791.2], pred [1795.87457463], mean 0.002609744659991645\n",
            "drift detect at index 248\n",
            "actual [1805.2], pred [1800.16445966], mean 0.0027894639582694944\n",
            "drift detect at index 249\n",
            "actual [1812.3], pred [1804.37280906], mean 0.0043741052455925035\n",
            "drift detect at index 250\n",
            "actual [1813.7], pred [1811.03190884], mean 0.0014710763417547514\n",
            "drift detect at index 251\n",
            "actual [1807.2], pred [1811.51643185], mean 0.0023884638407329173\n",
            "drift detect at index 252\n",
            "actual [1815.5], pred [1809.9831445], mean 0.003038752682866074\n",
            "drift detect at index 253\n",
            "actual [1798.1], pred [1807.06566106], mean 0.00498618600501697\n",
            "drift detect at index 254\n",
            "actual [1789.7], pred [1801.7258306], mean 0.006719467287135436\n",
            "drift detect at index 255\n",
            "actual [1776.7], pred [1785.33482584], mean 0.004860035934073548\n",
            "drift detect at index 256\n",
            "actual [1771.2], pred [1778.90329005], mean 0.004349192664414456\n",
            "drift detect at index 257\n",
            "actual [1762.9], pred [1767.60331891], mean 0.0026679442478235497\n",
            "drift detect at index 258\n",
            "actual [1748.4], pred [1757.71471518], mean 0.0053275653031228136\n",
            "drift detect at index 259\n",
            "actual [1761.2], pred [1754.45400653], mean 0.0038303392430585363\n",
            "drift detect at index 260\n",
            "actual [1761.5], pred [1757.46370085], mean 0.0022913988926731316\n",
            "drift detect at index 261\n",
            "actual [1771.4], pred [1764.14753499], mean 0.004094199510695187\n",
            "drift detect at index 262\n",
            "actual [1749.8], pred [1755.3905412], mean 0.00319496010975877\n",
            "drift detect at index 263\n",
            "actual [1749.7], pred [1753.62993661], mean 0.002246063101916626\n",
            "drift detect at index 264\n",
            "actual [1736.3], pred [1743.21767482], mean 0.003984147220610355\n",
            "drift detect at index 265\n",
            "actual [1726.2], pred [1732.86949837], mean 0.0038636880824188874\n",
            "drift detect at index 266\n",
            "actual [1709.3], pred [1729.32011741], mean 0.01171246557383099\n",
            "drift detect at index 267\n",
            "actual [1722.6], pred [1730.15117738], mean 0.004383593046272482\n",
            "drift detect at index 268\n",
            "actual [1712.9], pred [1735.92060411], mean 0.013439549365548946\n",
            "drift detect at index 269\n",
            "actual [1727.8], pred [1719.26299139], mean 0.004940970374784491\n",
            "drift detect at index 270\n",
            "actual [1720.2], pred [1738.07746866], mean 0.010392668680330449\n",
            "drift detect at index 271\n",
            "actual [1728.6], pred [1723.89233431], mean 0.0027233979490371464\n",
            "drift detect at index 272\n",
            "actual [1740.6], pred [1733.65382571], mean 0.003990678093978166\n",
            "drift detect at index 273\n",
            "actual [1717.4], pred [1742.72420854], mean 0.014745667020784827\n",
            "drift detect at index 274\n",
            "actual [1709.1], pred [1712.62157661], mean 0.0020604859907637227\n",
            "drift detect at index 275\n",
            "actual [1677.3], pred [1813.67422661], mean 0.08130580493269571\n",
            "drift detect at index 276\n",
            "actual [1683.5], pred [1701.08230392], mean 0.010443898974928072\n",
            "drift detect at index 277\n",
            "actual [1678.2], pred [1756.59246501], mean 0.04671223037260182\n",
            "drift detect at index 278\n",
            "actual [1671.1], pred [1747.68664385], mean 0.045830078304333485\n",
            "drift detect at index 279\n",
            "actual [1675.7], pred [1792.65520828], mean 0.06979483695444309\n",
            "drift detect at index 280\n",
            "actual [1681.1], pred [1769.36570637], mean 0.05250473283765095\n",
            "drift detect at index 281\n",
            "actual [1655.6], pred [1863.95595609], mean 0.12584921242587085\n",
            "drift detect at index 282\n",
            "actual [1633.4], pred [1792.76615795], mean 0.09756713478275593\n",
            "drift detect at index 283\n",
            "actual [1636.2], pred [1905.01087435], mean 0.16428974107814126\n",
            "drift detect at index 284\n",
            "actual [1670.], pred [2130.63834964], mean 0.2758313470908085\n",
            "drift detect at index 285\n",
            "actual [1668.6], pred [1931.21918259], mean 0.15738893838442228\n",
            "drift detect at index 286\n",
            "actual [1672.], pred [1839.26159463], mean 0.10003683889506905\n",
            "drift detect at index 287\n",
            "actual [1702.], pred [1812.84557418], mean 0.06512665933116903\n",
            "drift detect at index 288\n",
            "actual [1730.5], pred [1764.18638074], mean 0.019466270290209683\n",
            "drift detect at index 289\n",
            "actual [1720.8], pred [1725.86409527], mean 0.0029428726600648153\n",
            "drift detect at index 290\n",
            "actual [1720.9], pred [1724.90841996], mean 0.00232925792294724\n",
            "drift detect at index 291\n",
            "actual [1709.3], pred [1718.29683237], mean 0.00526346011457022\n",
            "drift detect at index 292\n",
            "actual [1675.2], pred [1718.22319081], mean 0.025682420491271993\n",
            "drift detect at index 293\n",
            "actual [1686.], pred [1690.0600612], mean 0.002408102727416991\n",
            "drift detect at index 294\n",
            "actual [1677.5], pred [1751.69558191], mean 0.044229855086816934\n",
            "drift detect at index 295\n",
            "actual [1672.9], pred [1761.06616686], mean 0.052702592422036905\n",
            "drift detect at index 296\n",
            "actual [1672.9], pred [1822.50020755], mean 0.08942567251440516\n",
            "drift detect at index 297\n",
            "actual [1664.], pred [1845.40521874], mean 0.10901755933625072\n",
            "drift detect at index 298\n",
            "actual [1655.5], pred [1843.21485523], mean 0.11338861687068078\n",
            "drift detect at index 299\n",
            "actual [1634.2], pred [2101.03713142], mean 0.2856670734454088\n",
            "drift detect at index 300\n",
            "actual [1636.8], pred [1819.52206222], mean 0.11163371347760018\n",
            "drift detect at index 301\n",
            "actual [1656.3], pred [2159.5730623], mean 0.30385380806811024\n",
            "drift detect at index 302\n",
            "actual [1654.1], pred [1925.51876219], mean 0.16408848448559812\n",
            "drift detect at index 303\n",
            "actual [1658.], pred [1791.14424893], mean 0.080304130839357\n",
            "drift detect at index 304\n",
            "actual [1669.2], pred [1872.91895886], mean 0.12204586560114052\n",
            "drift detect at index 305\n",
            "actual [1668.8], pred [1827.11610123], mean 0.09486822940434508\n",
            "drift detect at index 306\n",
            "actual [1648.3], pred [1914.58038683], mean 0.16154849653014833\n",
            "drift detect at index 307\n",
            "actual [1648.3], pred [1744.02531982], mean 0.05807518037744962\n",
            "drift detect at index 308\n",
            "actual [1636.4], pred [2167.56659323], mean 0.3245945937595422\n",
            "drift detect at index 309\n",
            "actual [1651.], pred [1934.65475279], mean 0.17180784542221958\n",
            "drift detect at index 310\n",
            "actual [1637.7], pred [1962.31320867], mean 0.19821286479119885\n",
            "drift detect at index 311\n",
            "actual [1685.7], pred [2138.21363128], mean 0.26844256467875943\n",
            "drift detect at index 312\n",
            "actual [1680.5], pred [1839.74140825], mean 0.09475835063739137\n",
            "drift detect at index 313\n",
            "actual [1716.], pred [1884.39690008], mean 0.09813339165719623\n",
            "drift detect at index 314\n",
            "actual [1715.8], pred [1758.39188084], mean 0.024823336544591706\n",
            "drift detect at index 315\n",
            "actual [1753.7], pred [1780.28390607], mean 0.015158753529728243\n",
            "drift detect at index 316\n",
            "actual [1774.2], pred [1757.15964994], mean 0.009604526015204893\n",
            "drift detect at index 317\n",
            "actual [1774.2], pred [1770.68867979], mean 0.001979100559179634\n",
            "drift detect at index 318\n",
            "actual [1774.7], pred [1775.98308188], mean 0.000722985225867998\n",
            "drift detect at index 319\n",
            "actual [1775.8], pred [1772.46948481], mean 0.0018755012903244987\n",
            "drift detect at index 320\n",
            "actual [1763.], pred [1770.94501551], mean 0.004506531768283584\n",
            "drift detect at index 321\n",
            "actual [1769.], pred [1768.38147562], mean 0.0003496463421831686\n",
            "drift detect at index 322\n",
            "actual [1754.6], pred [1765.28690817], mean 0.006090794578250133\n",
            "drift detect at index 323\n",
            "actual [1754.8], pred [1757.69195217], mean 0.0016480238013706955\n",
            "drift detect at index 324\n",
            "actual [1760.4], pred [1755.37322259], mean 0.002855474559005607\n",
            "drift detect at index 325\n",
            "actual [1768.8], pred [1764.63698221], mean 0.0023535831024363873\n",
            "drift detect at index 326\n",
            "actual [1755.3], pred [1758.59784951], mean 0.0018787953700287372\n",
            "drift detect at index 327\n",
            "actual [1763.7], pred [1761.34481233], mean 0.0013353675042454582\n",
            "drift detect at index 328\n",
            "actual [1759.9], pred [1757.11856253], mean 0.001580451996016205\n",
            "drift detect at index 329\n",
            "actual [1815.2], pred [1790.97174688], mean 0.013347429000432771\n",
            "drift detect at index 330\n",
            "actual [1809.6], pred [1806.92973657], mean 0.001475609764472622\n",
            "drift detect at index 331\n",
            "actual [1781.3], pred [1807.00100155], mean 0.014428227447052945\n",
            "drift detect at index 332\n",
            "actual [1782.4], pred [1794.78135229], mean 0.0069464498953977945\n",
            "drift detect at index 333\n",
            "actual [1798.], pred [1786.69608914], mean 0.006286935963985707\n",
            "drift detect at index 334\n",
            "actual [1801.5], pred [1796.11400233], mean 0.002989729484740229\n",
            "drift detect at index 335\n",
            "actual [1810.7], pred [1803.73961983], mean 0.003844027263791915\n",
            "drift detect at index 336\n",
            "actual [1792.3], pred [1803.09295112], mean 0.006021844065365196\n",
            "drift detect at index 337\n",
            "actual [1825.5], pred [1803.18117215], mean 0.01222614508431407\n",
            "drift detect at index 338\n",
            "actual [1818.7], pred [1822.16043762], mean 0.0019026984216054359\n",
            "drift detect at index 339\n",
            "actual [1787.8], pred [1813.28747404], mean 0.014256334066892345\n",
            "drift detect at index 340\n",
            "actual [1800.2], pred [1799.69444096], mean 0.0002808349276847644\n",
            "drift detect at index 341\n",
            "actual [1797.7], pred [1795.21239626], mean 0.0013837702274611813\n",
            "drift detect at index 342\n",
            "actual [1825.4], pred [1807.45660411], mean 0.009829843263004137\n",
            "drift detect at index 343\n",
            "actual [1823.9], pred [1823.90989591], mean 5.425685569037475e-06\n",
            "actual [1795.3], pred [1816.74999726], mean 0.011947862340583081\n",
            "drift detect at index 345\n",
            "actual [1804.2], pred [1807.2564328], mean 0.001694065401559653\n",
            "drift detect at index 346\n",
            "actual [1823.1], pred [1809.74982237], mean 0.007322789551649287\n",
            "drift detect at index 347\n",
            "actual [1815.8], pred [1822.60310729], mean 0.003746617079356154\n",
            "drift detect at index 348\n",
            "actual [1826.], pred [1819.36520791], mean 0.003633511549157207\n",
            "drift detect at index 349\n",
            "actual [1826.2], pred [1822.48326224], mean 0.0020352304033087122\n",
            "drift detect at index 350\n",
            "actual [1846.1], pred [1831.5696011], mean 0.007870862305519121\n",
            "drift detect at index 351\n",
            "actual [1859.], pred [1848.7231942], mean 0.0055281365239786655\n",
            "drift detect at index 352\n",
            "actual [1840.6], pred [1848.92542067], mean 0.00452321018827301\n",
            "drift detect at index 353\n",
            "actual [1869.7], pred [1852.40327738], mean 0.009251068416012451\n",
            "drift detect at index 354\n",
            "actual [1877.8], pred [1871.05725759], mean 0.00359076707129894\n",
            "drift detect at index 355\n",
            "actual [1876.5], pred [1873.44778054], mean 0.001626549139455179\n",
            "drift detect at index 356\n",
            "actual [1878.9], pred [1873.83886567], mean 0.002693668814051747\n",
            "drift detect at index 357\n",
            "actual [1898.8], pred [1888.90349236], mean 0.005211980006970134\n",
            "drift detect at index 358\n",
            "actual [1921.7], pred [1907.93898968], mean 0.007160852534571359\n",
            "drift detect at index 359\n",
            "actual [1909.9], pred [1908.30213875], mean 0.0008366203753332749\n",
            "drift detect at index 360\n",
            "actual [1907.], pred [1903.78357178], mean 0.001686643011666098\n",
            "drift detect at index 361\n",
            "actual [1923.9], pred [1913.88167373], mean 0.005207300936779268\n",
            "drift detect at index 362\n",
            "actual [1928.2], pred [1919.51192828], mean 0.004505793861690449\n",
            "drift detect at index 363\n",
            "actual [1945.4], pred [1933.13444955], mean 0.006304898967347274\n",
            "drift detect at index 364\n",
            "actual [1952.2], pred [1943.8666652], mean 0.004268689069608268\n",
            "drift detect at index 365\n",
            "actual [1959.4], pred [1950.37043486], mean 0.00460833170189358\n",
            "drift detect at index 366\n",
            "actual [1946.7], pred [1941.66721729], mean 0.0025852893129919598\n",
            "drift detect at index 367\n",
            "actual [1945.6], pred [1939.11627159], mean 0.003332508434140037\n",
            "drift detect at index 368\n",
            "actual [1939.2], pred [1934.03312325], mean 0.002664437268008992\n",
            "drift detect at index 369\n",
            "actual [1945.3], pred [1942.10267743], mean 0.001643614130823695\n",
            "drift detect at index 370\n",
            "actual [1942.8], pred [1933.890573], mean 0.004585869365860196\n",
            "drift detect at index 371\n",
            "actual [1930.8], pred [1937.43285753], mean 0.0034352897899107403\n",
            "drift detect at index 372\n",
            "actual [1876.6], pred [1942.4988741], mean 0.03511610044601739\n",
            "drift detect at index 373\n",
            "actual [1879.5], pred [1877.33404762], mean 0.001152408821060606\n",
            "drift detect at index 374\n",
            "actual [1884.8], pred [1879.4887963], mean 0.0028179136764418197\n",
            "drift detect at index 375\n",
            "actual [1890.7], pred [1886.19128077], mean 0.00238468251503968\n",
            "drift detect at index 376\n",
            "actual [1878.5], pred [1876.82716283], mean 0.0008905175266386159\n",
            "drift detect at index 377\n",
            "actual [1874.5], pred [1874.5634784], mean 3.386417633728898e-05\n",
            "drift detect at index 378\n",
            "actual [1863.5], pred [1866.04392065], mean 0.001365130481766653\n",
            "drift detect at index 379\n",
            "actual [1865.4], pred [1863.16909187], mean 0.001195940886434311\n",
            "drift detect at index 380\n",
            "actual [1845.3], pred [1856.03229933], mean 0.00581601871484601\n",
            "drift detect at index 381\n",
            "actual [1851.8], pred [1850.12568045], mean 0.0009041578710070239\n",
            "drift detect at index 382\n",
            "actual [1850.2], pred [1847.79692391], mean 0.0012988196379480018\n",
            "drift detect at index 383\n",
            "actual [1842.5], pred [1844.19038102], mean 0.0009174388170529665\n",
            "drift detect at index 384\n",
            "actual [1841.5], pred [1842.31253049], mean 0.00044123295657590985\n",
            "drift detect at index 385\n",
            "actual [1826.8], pred [1836.15552329], mean 0.005121263022526277\n",
            "drift detect at index 386\n",
            "actual [1817.1], pred [1824.75082138], mean 0.004210456981467112\n",
            "drift detect at index 387\n",
            "actual [1824.9], pred [1822.81135243], mean 0.0011445271337874945\n",
            "drift detect at index 388\n",
            "actual [1836.7], pred [1826.81652665], mean 0.005381103800461392\n",
            "drift detect at index 389\n",
            "actual [1845.4], pred [1836.91163957], mean 0.0045997401259251615\n",
            "drift detect at index 390\n",
            "actual [1840.5], pred [1841.60291085], mean 0.0005992452304802678\n",
            "drift detect at index 391\n",
            "actual [1854.6], pred [1845.43052746], mean 0.004944178009918163\n",
            "drift detect at index 392\n",
            "actual [1852.4], pred [1852.408734], mean 4.714966342945156e-06\n",
            "actual [1820.], pred [1846.19862759], mean 0.014394850324608147\n",
            "drift detect at index 394\n",
            "actual [1818.6], pred [1829.07120802], mean 0.005757840108916178\n",
            "drift detect at index 395\n",
            "actual [1834.6], pred [1822.37222267], mean 0.006665091754786952\n",
            "drift detect at index 396\n",
            "actual [1867.2], pred [1832.09234141], mean 0.018802302160678817\n",
            "drift detect at index 397\n",
            "actual [1916.5], pred [1855.43711123], mean 0.031861669067762576\n",
            "drift detect at index 398\n",
            "actual [1910.9], pred [1906.04724785], mean 0.002539511301321088\n",
            "drift detect at index 399\n",
            "actual [1931.3], pred [1927.90808416], mean 0.0017562863543902657\n",
            "drift detect at index 400\n",
            "actual [1923.], pred [1918.1774255], mean 0.002507839051278578\n",
            "drift detect at index 401\n",
            "actual [1990.2], pred [1920.07961593], mean 0.035232832915611766\n",
            "drift detect at index 402\n",
            "actual [1999.7], pred [1985.85824937], mean 0.006921913603304341\n",
            "drift detect at index 403\n",
            "actual [1941.1], pred [1991.94739024], mean 0.026195142055783528\n",
            "drift detect at index 404\n",
            "actual [1949.6], pred [1944.90764059], mean 0.0024068318667010064\n",
            "drift detect at index 405\n",
            "actual [1995.9], pred [1990.02303659], mean 0.002944517965618256\n",
            "drift detect at index 406\n",
            "actual [1983.8], pred [1991.91671265], mean 0.004091497455183567\n",
            "drift detect at index 407\n",
            "actual [1953.8], pred [1985.06112947], mean 0.016000168629792512\n",
            "drift detect at index 408\n",
            "actual [1973.5], pred [1961.54743517], mean 0.006056531454400219\n",
            "drift detect at index 409\n",
            "actual [1966.9], pred [1960.50844165], mean 0.003249559383635084\n",
            "drift detect at index 410\n",
            "actual [1980.3], pred [1986.86178858], mean 0.0033135325868330627\n",
            "drift detect at index 411\n",
            "actual [1969.], pred [1985.56038411], mean 0.008410555669925707\n",
            "drift detect at index 412\n",
            "actual [1983.9], pred [1995.99807804], mean 0.006098128957364991\n",
            "drift detect at index 413\n",
            "actual [2022.2], pred [2004.48555668], mean 0.008759985815328395\n",
            "drift detect at index 414\n",
            "actual [2020.9], pred [2041.57860889], mean 0.01023237611434963\n",
            "drift detect at index 415\n",
            "actual [2026.4], pred [2024.58805421], mean 0.0008941698532178667\n",
            "drift detect at index 416\n",
            "actual [2003.8], pred [2018.87121529], mean 0.007521317141110309\n",
            "drift detect at index 417\n",
            "actual [2019.], pred [2011.14976228], mean 0.003888181139024199\n",
            "drift detect at index 418\n",
            "actual [2024.9], pred [2029.00354737], mean 0.0020265432216579264\n",
            "drift detect at index 419\n",
            "actual [2055.3], pred [2053.74987802], mean 0.0007542071602316837\n",
            "drift detect at index 420\n",
            "actual [2015.8], pred [2046.28981228], mean 0.015125415357203924\n",
            "drift detect at index 421\n",
            "actual [2007.], pred [2012.74510344], mean 0.0028625328572119235\n",
            "drift detect at index 422\n",
            "actual [2019.7], pred [2013.16735469], mean 0.003234463192096237\n",
            "drift detect at index 423\n",
            "actual [2007.3], pred [2022.49721723], mean 0.007570974558818681\n",
            "drift detect at index 424\n",
            "actual [2019.1], pred [2006.21654742], mean 0.006380789746577675\n",
            "drift detect at index 425\n",
            "actual [1990.5], pred [2020.25725659], mean 0.014949639080696745\n",
            "drift detect at index 426\n",
            "actual [1999.8], pred [1990.78102398], mean 0.004509939003350018\n",
            "drift detect at index 427\n",
            "actual [2004.5], pred [2006.49139877], mean 0.0009934640885848939\n",
            "drift detect at index 428\n",
            "actual [1996.], pred [2006.65923024], mean 0.00534029571103733\n",
            "drift detect at index 429\n",
            "actual [1999.], pred [2002.8648831], mean 0.0019334082556385114\n",
            "drift detect at index 430\n",
            "actual [1999.1], pred [1997.07809674], mean 0.0010114067645182329\n",
            "drift detect at index 431\n",
            "actual [1992.2], pred [1996.11200666], mean 0.0019636616123386206\n",
            "drift detect at index 432\n",
            "actual [2023.3], pred [2020.19949946], mean 0.0015323978350614553\n",
            "drift detect at index 433\n",
            "actual [2037.], pred [2038.82813063], mean 0.0008974622607612251\n",
            "drift detect at index 434\n",
            "actual [2055.7], pred [2059.60611155], mean 0.0019001369583809505\n",
            "drift detect at index 435\n",
            "actual [2024.8], pred [2048.31305879], mean 0.011612533976042463\n",
            "drift detect at index 436\n",
            "actual [2033.2], pred [2021.35757768], mean 0.005824524059535647\n",
            "drift detect at index 437\n",
            "actual [2042.9], pred [2043.14680602], mean 0.00012081160103563324\n",
            "drift detect at index 438\n",
            "actual [2037.1], pred [2048.31389671], mean 0.005504833690356958\n",
            "drift detect at index 439\n",
            "actual [2020.5], pred [2037.04258423], mean 0.00818737155487659\n",
            "drift detect at index 440\n",
            "actual [2019.8], pred [2025.91058603], mean 0.0030253421271675906\n",
            "drift detect at index 441\n",
            "actual [2022.7], pred [2020.70938402], mean 0.000984138024935849\n",
            "drift detect at index 442\n",
            "actual [1993.], pred [2026.00752056], mean 0.016561726324401692\n",
            "drift detect at index 443\n",
            "actual [1984.9], pred [1995.34947878], mean 0.005264486263464179\n",
            "drift detect at index 444\n",
            "actual [1959.8], pred [1988.09659265], mean 0.014438510381241587\n",
            "drift detect at index 445\n",
            "actual [1981.6], pred [1973.74673614], mean 0.0039630923815034795\n",
            "drift detect at index 446\n",
            "actual [1977.2], pred [1970.55358715], mean 0.003361527844436296\n",
            "drift detect at index 447\n",
            "actual [1974.5], pred [1981.45388403], mean 0.003521845543538561\n",
            "drift detect at index 448\n",
            "actual [1964.6], pred [1972.05429546], mean 0.003794306962049296\n",
            "drift detect at index 449\n",
            "actual [1943.7], pred [1963.74454761], mean 0.010312572726521656\n",
            "drift detect at index 450\n",
            "actual [1944.3], pred [1940.1262074], mean 0.0021466813770695827\n",
            "drift detect at index 451\n",
            "actual [1958.], pred [1950.49093858], mean 0.0038350671206608964\n",
            "drift detect at index 452\n",
            "actual [1963.9], pred [1963.71941998], mean 9.194970085035178e-05\n",
            "drift detect at index 453\n",
            "actual [1978.], pred [1981.86788369], mean 0.0019554518134809844\n",
            "drift detect at index 454\n",
            "actual [1952.4], pred [1970.72099076], mean 0.009383830545614957\n",
            "drift detect at index 455\n",
            "actual [1958.], pred [1973.93369006], mean 0.008137737515663834\n",
            "drift detect at index 456\n",
            "actual [1965.5], pred [1968.59234458], mean 0.00157331192112492\n",
            "drift detect at index 457\n",
            "actual [1958.4], pred [1961.92946742], mean 0.0018022198861177815\n",
            "drift detect at index 458\n",
            "actual [1978.6], pred [1970.35379085], mean 0.004167698954461965\n",
            "drift detect at index 459\n",
            "actual [1977.2], pred [1967.3897906], mean 0.004961667711852515\n",
            "drift detect at index 460\n",
            "actual [1969.7], pred [1968.73514643], mean 0.0004898479829875275\n",
            "drift detect at index 461\n",
            "actual [1958.6], pred [1967.81322265], mean 0.004703983787043386\n",
            "drift detect at index 462\n",
            "actual [1968.9], pred [1964.67023731], mean 0.002148287212557555\n",
            "drift detect at index 463\n",
            "actual [1970.7], pred [1972.28739365], mean 0.000805497360993268\n",
            "drift detect at index 464\n",
            "actual [1971.2], pred [1957.33617371], mean 0.007033191094845097\n",
            "drift detect at index 465\n",
            "actual [1947.7], pred [1972.41176852], mean 0.012687666747834583\n",
            "drift detect at index 466\n",
            "actual [1944.9], pred [1942.68203054], mean 0.0011404028298512843\n",
            "drift detect at index 467\n",
            "actual [1923.7], pred [1928.18778741], mean 0.0023328935935584705\n",
            "drift detect at index 468\n",
            "actual [1929.6], pred [1925.09545151], mean 0.002334446769044811\n",
            "drift detect at index 469\n",
            "actual [1933.8], pred [1926.40209837], mean 0.0038255774296283786\n",
            "drift detect at index 470\n",
            "actual [1923.8], pred [1922.95981455], mean 0.0004367322230077748\n",
            "drift detect at index 471\n",
            "actual [1922.2], pred [1917.65930951], mean 0.002362236232733169\n",
            "drift detect at index 472\n",
            "actual [1917.9], pred [1912.81792881], mean 0.002649810308910693\n",
            "drift detect at index 473\n",
            "actual [1929.4], pred [1920.260712], mean 0.0047368549826836606\n",
            "drift detect at index 474\n",
            "actual [1929.5], pred [1920.85885118], mean 0.00447843940134716\n",
            "drift detect at index 475\n",
            "actual [1927.1], pred [1924.54020386], mean 0.0013283151552594473\n",
            "drift detect at index 476\n",
            "actual [1915.4], pred [1918.06743181], mean 0.0013926238933577324\n",
            "drift detect at index 477\n",
            "actual [1932.5], pred [1923.85190971], mean 0.004475079062810177\n",
            "drift detect at index 478\n",
            "actual [1931.], pred [1924.30511257], mean 0.0034670571905329414\n",
            "drift detect at index 479\n",
            "actual [1937.1], pred [1930.0252539], mean 0.0036522358706242123\n",
            "drift detect at index 480\n",
            "actual [1961.7], pred [1954.91861532], mean 0.003456891818349923\n",
            "drift detect at index 481\n",
            "actual [1963.8], pred [1958.43218579], mean 0.00273338130572782\n",
            "drift detect at index 482\n",
            "actual [1964.4], pred [1964.25779339], mean 7.239187912922965e-05\n",
            "drift detect at index 483\n",
            "actual [1956.4], pred [1956.43759935], mean 1.9218639690741998e-05\n",
            "warning level at index 484\n",
            "actual [1980.8], pred [1976.90626313], mean 0.0019657395355575412\n",
            "drift detect at index 485\n",
            "actual [1980.4], pred [1972.67689604], mean 0.003899769721828449\n",
            "drift detect at index 486\n",
            "actual [1970.9], pred [1979.80215568], mean 0.004516797240051171\n",
            "drift detect at index 487\n",
            "actual [1966.6], pred [1966.68195886], mean 4.167540883400639e-05\n",
            "drift detect at index 488\n",
            "actual [1962.2], pred [1965.3110528], mean 0.0015854922029822766\n",
            "drift detect at index 489\n",
            "actual [1963.7], pred [1962.23863792], mean 0.0007441880521221899\n",
            "drift detect at index 490\n",
            "actual [1970.1], pred [1967.52366493], mean 0.001307717918386884\n",
            "drift detect at index 491\n",
            "actual [1945.7], pred [1966.67038464], mean 0.010777809857636197\n",
            "drift detect at index 492\n",
            "actual [1960.4], pred [1958.93591811], mean 0.0007468281410774923\n",
            "drift detect at index 493\n",
            "actual [1970.5], pred [1972.73747345], mean 0.0011354851316080657\n",
            "drift detect at index 494\n",
            "actual [1940.7], pred [1957.99001488], mean 0.008909164155007471\n",
            "drift detect at index 495\n",
            "actual [1937.4], pred [1956.43685409], mean 0.009825980224614297\n",
            "drift detect at index 496\n",
            "actual [1932.], pred [1946.4135918], mean 0.007460451240649561\n",
            "drift detect at index 497\n",
            "actual [1939.6], pred [1942.17457458], mean 0.0013273739850010674\n",
            "drift detect at index 498\n",
            "actual [1970.], pred [1941.59781407], mean 0.014417353262398751\n",
            "drift detect at index 499\n",
            "actual [1959.9], pred [1961.95934368], mean 0.0010507391591566687\n",
            "drift detect at index 500\n",
            "actual [1950.6], pred [1950.99955086], mean 0.00020483485061212212\n",
            "drift detect at index 501\n",
            "actual [1948.9], pred [1946.54557505], mean 0.001208078888310114\n",
            "drift detect at index 502\n",
            "actual [1946.6], pred [1942.57643192], mean 0.0020669721970431156\n",
            "drift detect at index 503\n",
            "actual [1944.], pred [1940.54561574], mean 0.0017769466349260076\n",
            "drift detect at index 504\n",
            "actual [1935.2], pred [1932.29525662], mean 0.0015010042245742236\n",
            "drift detect at index 505\n",
            "actual [1928.3], pred [1924.89927253], mean 0.0017635883805803174\n",
            "drift detect at index 506\n",
            "actual [1915.2], pred [1913.75661079], mean 0.0007536493383086826\n",
            "drift detect at index 507\n"
          ]
        }
      ]
    }
  ]
}